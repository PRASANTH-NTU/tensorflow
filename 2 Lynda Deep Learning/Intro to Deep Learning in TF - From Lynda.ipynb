{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": "true"
   },
   "source": [
    " # Table of Contents\n",
    "<div class=\"toc\" style=\"margin-top: 1em;\"><ul class=\"toc-item\" id=\"toc-level0\"><li><span><a href=\"http://localhost:8888/notebooks/Dropbox/Python%20Folder/Lynda%20-%20Deep%20Learning%20in%20TF/Intro%20to%20Deep%20Learning%20in%20TF%20-%20From%20Lynda.ipynb#Setting-up-TensorFlow\" data-toc-modified-id=\"Setting-up-TensorFlow-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Setting up TensorFlow</a></span></li><li><span><a href=\"http://localhost:8888/notebooks/Dropbox/Python%20Folder/Lynda%20-%20Deep%20Learning%20in%20TF/Intro%20to%20Deep%20Learning%20in%20TF%20-%20From%20Lynda.ipynb#Tensor-Flow-Overview\" data-toc-modified-id=\"Tensor-Flow-Overview-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Tensor Flow Overview</a></span><ul class=\"toc-item\"><li><span><a href=\"http://localhost:8888/notebooks/Dropbox/Python%20Folder/Lynda%20-%20Deep%20Learning%20in%20TF/Intro%20to%20Deep%20Learning%20in%20TF%20-%20From%20Lynda.ipynb#addition-final.py\" data-toc-modified-id=\"addition-final.py-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>addition final.py</a></span></li></ul></li><li><span><a href=\"http://localhost:8888/notebooks/Dropbox/Python%20Folder/Lynda%20-%20Deep%20Learning%20in%20TF/Intro%20to%20Deep%20Learning%20in%20TF%20-%20From%20Lynda.ipynb#Creating-a-TensorFlow-Model\" data-toc-modified-id=\"Creating-a-TensorFlow-Model-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Creating a TensorFlow Model</a></span><ul class=\"toc-item\"><ul class=\"toc-item\"><li><span><a href=\"http://localhost:8888/notebooks/Dropbox/Python%20Folder/Lynda%20-%20Deep%20Learning%20in%20TF/Intro%20to%20Deep%20Learning%20in%20TF%20-%20From%20Lynda.ipynb#Story-line\" data-toc-modified-id=\"Story-line-3.0.1\"><span class=\"toc-item-num\">3.0.1&nbsp;&nbsp;</span>Story line</a></span><ul class=\"toc-item\"><li><span><a href=\"http://localhost:8888/notebooks/Dropbox/Python%20Folder/Lynda%20-%20Deep%20Learning%20in%20TF/Intro%20to%20Deep%20Learning%20in%20TF%20-%20From%20Lynda.ipynb#Data-Preprocessing:-Loading-&amp;-cleaning-data\" data-toc-modified-id=\"Data-Preprocessing:-Loading-&amp;-cleaning-data-3.0.1.1\"><span class=\"toc-item-num\">3.0.1.1&nbsp;&nbsp;</span><strong>Data Preprocessing: Loading &amp; cleaning data</strong></a></span></li><li><span><a href=\"http://localhost:8888/notebooks/Dropbox/Python%20Folder/Lynda%20-%20Deep%20Learning%20in%20TF/Intro%20to%20Deep%20Learning%20in%20TF%20-%20From%20Lynda.ipynb#Build-a-Neural-network-model\" data-toc-modified-id=\"Build-a-Neural-network-model-3.0.1.2\"><span class=\"toc-item-num\">3.0.1.2&nbsp;&nbsp;</span><strong>Build a Neural network model</strong></a></span></li><li><span><a href=\"http://localhost:8888/notebooks/Dropbox/Python%20Folder/Lynda%20-%20Deep%20Learning%20in%20TF/Intro%20to%20Deep%20Learning%20in%20TF%20-%20From%20Lynda.ipynb#Train-the-network\" data-toc-modified-id=\"Train-the-network-3.0.1.3\"><span class=\"toc-item-num\">3.0.1.3&nbsp;&nbsp;</span><strong>Train the network</strong></a></span></li><li><span><a href=\"http://localhost:8888/notebooks/Dropbox/Python%20Folder/Lynda%20-%20Deep%20Learning%20in%20TF/Intro%20to%20Deep%20Learning%20in%20TF%20-%20From%20Lynda.ipynb#Predict-the-output\" data-toc-modified-id=\"Predict-the-output-3.0.1.4\"><span class=\"toc-item-num\">3.0.1.4&nbsp;&nbsp;</span><strong>Predict the output</strong></a></span></li><li><span><a href=\"http://localhost:8888/notebooks/Dropbox/Python%20Folder/Lynda%20-%20Deep%20Learning%20in%20TF/Intro%20to%20Deep%20Learning%20in%20TF%20-%20From%20Lynda.ipynb#Log-the-progress\" data-toc-modified-id=\"Log-the-progress-3.0.1.5\"><span class=\"toc-item-num\">3.0.1.5&nbsp;&nbsp;</span>Log the progress</a></span></li><li><span><a href=\"http://localhost:8888/notebooks/Dropbox/Python%20Folder/Lynda%20-%20Deep%20Learning%20in%20TF/Intro%20to%20Deep%20Learning%20in%20TF%20-%20From%20Lynda.ipynb#save-model-checkpoint\" data-toc-modified-id=\"save-model-checkpoint-3.0.1.6\"><span class=\"toc-item-num\">3.0.1.6&nbsp;&nbsp;</span>save model checkpoint</a></span></li><li><span><a href=\"http://localhost:8888/notebooks/Dropbox/Python%20Folder/Lynda%20-%20Deep%20Learning%20in%20TF/Intro%20to%20Deep%20Learning%20in%20TF%20-%20From%20Lynda.ipynb#load-model-checkpoint\" data-toc-modified-id=\"load-model-checkpoint-3.0.1.7\"><span class=\"toc-item-num\">3.0.1.7&nbsp;&nbsp;</span>load model checkpoint</a></span></li><li><span><a href=\"http://localhost:8888/notebooks/Dropbox/Python%20Folder/Lynda%20-%20Deep%20Learning%20in%20TF/Intro%20to%20Deep%20Learning%20in%20TF%20-%20From%20Lynda.ipynb#Tensorboard:-Visualise-the-Computational-Graph\" data-toc-modified-id=\"Tensorboard:-Visualise-the-Computational-Graph-3.0.1.8\"><span class=\"toc-item-num\">3.0.1.8&nbsp;&nbsp;</span>Tensorboard: Visualise the Computational Graph</a></span></li></ul></li></ul><li><span><a href=\"http://localhost:8888/notebooks/Dropbox/Python%20Folder/Lynda%20-%20Deep%20Learning%20in%20TF/Intro%20to%20Deep%20Learning%20in%20TF%20-%20From%20Lynda.ipynb#load-data-final.py\" data-toc-modified-id=\"load-data-final.py-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>load data final.py</a></span><ul class=\"toc-item\"><li><span><a href=\"http://localhost:8888/notebooks/Dropbox/Python%20Folder/Lynda%20-%20Deep%20Learning%20in%20TF/Intro%20to%20Deep%20Learning%20in%20TF%20-%20From%20Lynda.ipynb#MinMaxScaler\" data-toc-modified-id=\"MinMaxScaler-3.1.1\"><span class=\"toc-item-num\">3.1.1&nbsp;&nbsp;</span><strong>MinMaxScaler</strong></a></span></li></ul></li><li><span><a href=\"http://localhost:8888/notebooks/Dropbox/Python%20Folder/Lynda%20-%20Deep%20Learning%20in%20TF/Intro%20to%20Deep%20Learning%20in%20TF%20-%20From%20Lynda.ipynb#model-final.py\" data-toc-modified-id=\"model-final.py-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>model final.py</a></span></li><li><span><a href=\"http://localhost:8888/notebooks/Dropbox/Python%20Folder/Lynda%20-%20Deep%20Learning%20in%20TF/Intro%20to%20Deep%20Learning%20in%20TF%20-%20From%20Lynda.ipynb#training-loop-final.py\" data-toc-modified-id=\"training-loop-final.py-3.3\"><span class=\"toc-item-num\">3.3&nbsp;&nbsp;</span>training loop final.py</a></span></li></ul></li><li><span><a href=\"http://localhost:8888/notebooks/Dropbox/Python%20Folder/Lynda%20-%20Deep%20Learning%20in%20TF/Intro%20to%20Deep%20Learning%20in%20TF%20-%20From%20Lynda.ipynb#Training-a-Model-in-TensorFlow\" data-toc-modified-id=\"Training-a-Model-in-TensorFlow-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Training a Model in TensorFlow</a></span><ul class=\"toc-item\"><li><span><a href=\"http://localhost:8888/notebooks/Dropbox/Python%20Folder/Lynda%20-%20Deep%20Learning%20in%20TF/Intro%20to%20Deep%20Learning%20in%20TF%20-%20From%20Lynda.ipynb#training-final.py\" data-toc-modified-id=\"training-final.py-4.1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;</span>training final.py</a></span></li><li><span><a href=\"http://localhost:8888/notebooks/Dropbox/Python%20Folder/Lynda%20-%20Deep%20Learning%20in%20TF/Intro%20to%20Deep%20Learning%20in%20TF%20-%20From%20Lynda.ipynb#model-logging-final.py\" data-toc-modified-id=\"model-logging-final.py-4.2\"><span class=\"toc-item-num\">4.2&nbsp;&nbsp;</span>model logging final.py</a></span></li><li><span><a href=\"http://localhost:8888/notebooks/Dropbox/Python%20Folder/Lynda%20-%20Deep%20Learning%20in%20TF/Intro%20to%20Deep%20Learning%20in%20TF%20-%20From%20Lynda.ipynb#Saving-&amp;-Loading-Models\" data-toc-modified-id=\"Saving-&amp;-Loading-Models-4.3\"><span class=\"toc-item-num\">4.3&nbsp;&nbsp;</span>Saving &amp; Loading Models</a></span><ul class=\"toc-item\"><li><span><a href=\"http://localhost:8888/notebooks/Dropbox/Python%20Folder/Lynda%20-%20Deep%20Learning%20in%20TF/Intro%20to%20Deep%20Learning%20in%20TF%20-%20From%20Lynda.ipynb#model-checkpoints-final.py\" data-toc-modified-id=\"model-checkpoints-final.py-4.3.1\"><span class=\"toc-item-num\">4.3.1&nbsp;&nbsp;</span>model checkpoints final.py</a></span></li><li><span><a href=\"http://localhost:8888/notebooks/Dropbox/Python%20Folder/Lynda%20-%20Deep%20Learning%20in%20TF/Intro%20to%20Deep%20Learning%20in%20TF%20-%20From%20Lynda.ipynb#load-checkpoint.py\" data-toc-modified-id=\"load-checkpoint.py-4.3.2\"><span class=\"toc-item-num\">4.3.2&nbsp;&nbsp;</span>load checkpoint.py</a></span></li></ul></li></ul></li><li><span><a href=\"http://localhost:8888/notebooks/Dropbox/Python%20Folder/Lynda%20-%20Deep%20Learning%20in%20TF/Intro%20to%20Deep%20Learning%20in%20TF%20-%20From%20Lynda.ipynb#TensorBoard\" data-toc-modified-id=\"TensorBoard-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>TensorBoard</a></span><ul class=\"toc-item\"><li><span><a href=\"http://localhost:8888/notebooks/Dropbox/Python%20Folder/Lynda%20-%20Deep%20Learning%20in%20TF/Intro%20to%20Deep%20Learning%20in%20TF%20-%20From%20Lynda.ipynb#train_model.py\" data-toc-modified-id=\"train_model.py-5.1\"><span class=\"toc-item-num\">5.1&nbsp;&nbsp;</span>train_model.py</a></span><ul class=\"toc-item\"><li><span><a href=\"http://localhost:8888/notebooks/Dropbox/Python%20Folder/Lynda%20-%20Deep%20Learning%20in%20TF/Intro%20to%20Deep%20Learning%20in%20TF%20-%20From%20Lynda.ipynb#Visualise-the-Computational-Graph\" data-toc-modified-id=\"Visualise-the-Computational-Graph-5.1.1\"><span class=\"toc-item-num\">5.1.1&nbsp;&nbsp;</span>Visualise the Computational Graph</a></span></li></ul></li><li><span><a href=\"http://localhost:8888/notebooks/Dropbox/Python%20Folder/Lynda%20-%20Deep%20Learning%20in%20TF/Intro%20to%20Deep%20Learning%20in%20TF%20-%20From%20Lynda.ipynb#visulize_training.py\" data-toc-modified-id=\"visulize_training.py-5.2\"><span class=\"toc-item-num\">5.2&nbsp;&nbsp;</span>visulize_training.py</a></span></li><li><span><a href=\"http://localhost:8888/notebooks/Dropbox/Python%20Folder/Lynda%20-%20Deep%20Learning%20in%20TF/Intro%20to%20Deep%20Learning%20in%20TF%20-%20From%20Lynda.ipynb#custom_viz.py\" data-toc-modified-id=\"custom_viz.py-5.3\"><span class=\"toc-item-num\">5.3&nbsp;&nbsp;</span>custom_viz.py</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting up TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Anaconda** - Provides pre-packaged versions of python machine learning libraries making installation easier\n",
    "    - https://www.anaconda.com/download/#macos\n",
    "    - https://www.anaconda.com/download/#windows\n",
    "    \n",
    "    \n",
    "**PyCharm** - IDE for creating & running Python projects\n",
    "    - https://www.jetbrains.com/pycharm/download/#section=mac\n",
    "    \n",
    "**TensorFlow**\n",
    "    - https://www.tensorflow.org/install/\n",
    "        \n",
    "**Libraries used explicitly**\n",
    "    - tensorflow \n",
    "    - pandas\n",
    "    - sklearn.preprocessing\n",
    "    - numpy\n",
    "    \n",
    "**Requirements (excplicit and implicit)**    \n",
    "    - numpy\n",
    "    - pip>=9.0.0\n",
    "    - pandas\n",
    "    - scikit-learn\n",
    "    - scipy\n",
    "    - tensorflow>=1.0.0\n",
    "    - google-api-python-client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Tensor Flow Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " **<font color=blue>What is TensorFlow?</font>**\n",
    " - Software package/libaray that allows to build machine learning models, especially to build Deep Neural Network\n",
    "\n",
    "**TensorFlow Alternatives**\n",
    " - Theano\n",
    " - Torch\n",
    " - Pytorch\n",
    " \n",
    "**Keras**\n",
    " - High level programming toolkit (*wrapper*) for 'TensorFlow'\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<font color=blue>Why it's called TensorFlow?</font>**\n",
    " - Tensors - Data are stored in *multidimensional arrays*\n",
    " - TensorFlow - How data/tensors flow through the system\n",
    " - **Computational Graph** - Simialr to flow chart\n",
    "     - In this graph, we are passing 2 data array/tensors, adding, squaring & outputting them as another data array/ tensor\n",
    "<img src=\"Images/Computational Graph 1.png\" width=\"400\" height=\"790\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<font color=blue>TensorFlow Requirements</font>**\n",
    " - Computers running Windows, Mac, Linux\n",
    " - Linux servers running TensorFlow Serving\n",
    " - Google's clould ML engine service\n",
    " - iOS or Andriod mobile devices/ apps\n",
    " \n",
    "**Programming Language Support**\n",
    " - Tf's core execution engine is developed in C++ for speed\n",
    " - *Python is the best supported & easiest* language to use with TensorFlow\n",
    " \n",
    " \n",
    " <font color=green>Though, tensorflow models are develped in Python, we still get great performance as majority of the heavy/ complex mathematical operations are performed in high performance exection engine built in C++</font>\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " **<font color=blue>Steps involved in Supervised Learning</font>**\n",
    "\n",
    " 1. Choose a Machine Learning Model - e.g. Neural Network\n",
    " 2. Training Phase \n",
    " 3. Testing Phase\n",
    " 4. Evalulation Phase\n",
    " \n",
    "Let's look at how each phase is implemented in TensorFlow:\n",
    " 1. Build a ML Model/Algo  as a Computational Graph- e.g. Neural Network\n",
    "  - Inputs, Outputs -> Placeholder\n",
    "  - Loss Function, Training -> Functions\n",
    "      - <img src=\"Images/Computational Graph 2.png\" width=\"300\" height=\"400\">\n",
    " 2. Training Phase\n",
    "   -  Before execution in graph, create a tf session\n",
    "   - **<font size=5 color = red>Session</font>** -> *A session is an object in tensorflow that runs operations on the graph and tracks the state of each node in the graph*\n",
    "     - <img src=\"Images/Computational Graph 2b.png\" width=\"400\" height=\"790\">\n",
    "   - **TensorBoard** -> While training process is running, we can watch the results graphically in real time using TensorBoard\n",
    "     - <img src=\"Images/Tensor Board 1.png\" width=\"500\" height=\"300\">\n",
    "     \n",
    " 3. Testing Phase \n",
    "   - After training the model, we pass in test data & measure the accuracy using Loss Function\n",
    "   - Once we have reached the expected accuracy, we can save this trained model\n",
    "       - <font color = green>i.e. We are storing the computational graph & state of all nodes in it</font>\n",
    "\n",
    " 4. Evaluation Phase \n",
    "   - After restoring the graph from saved model, we can make predictions on new data\n",
    "   - <font color = green>During evaulation phase, we only need neural network nodes. Loss function & Training model are no longer needed </font>\n",
    "   \n",
    "*Note: There's no start or end in a computational graph. We can start processing at any node in the graph*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## addition final.py\n",
    "\n",
    " - Computational Graph\n",
    "   - This graph has 2 input nodes (X, Y)\n",
    "   - It has one 'addition' operation node\n",
    "     <img src=\"Images/Addition Computational Graph 1.png\" width=\"250\" height=\"150\">\n",
    "   \n",
    " - Session\n",
    "   - Once tf computational graph is created, we will create a tensor flow session.\n",
    "   - Then, we input values for X,Y & we will ask the session object to execute the addition node\n",
    "     <img src=\"Images/Addition Session 1.png\" width=\"400\" height=\"300\"> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 5.]\n",
      "[  5.   4.  20.]\n",
      "[  5.   4.  20.]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "\n",
    "# Turn off TensorFlow warning messages in program output\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "\n",
    "# Define computational graph\n",
    "X = tf.placeholder(tf.float32, name=\"X\")\n",
    "Y = tf.placeholder(tf.float32, name=\"Y\")\n",
    "\n",
    "addition = tf.add(X, Y, name=\"addition\")\n",
    "addition2 = X + Y  # + provides a shortcut for tf.add(a, b)\n",
    "\n",
    "# Create the session to execute the graph\n",
    "with tf.Session() as session:\n",
    "\n",
    "    result1 = session.run(addition, feed_dict={X: [1], Y: [4]})\n",
    "    print(result1)\n",
    "    \n",
    "    result2 = session.run(addition, feed_dict={X: [1, 2, 10], Y: [4, 2, 10]})\n",
    "    print(result2)\n",
    "    \n",
    "    result3 = session.run(addition2, feed_dict={X: [1, 2, 10], Y: [4, 2, 10]})\n",
    "    print(result3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating a TensorFlow Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " **<font color=blue>Options for loading data into TensorFlow</font>**\n",
    " 1. **Preloading all data into memory** - Simpler method\n",
    "  - Quick, easy\n",
    "  - <font color=red> Words only if the entire dataset fits into RAM</font>\n",
    "  - Nothing tensorflow specific\n",
    "  - Uses normal python code & may use python data libraries such as *pandas* (esp for .csv files)\n",
    "<br><br>\n",
    " 2. **Feed data step-by-step** - Complicated version\n",
    "  - Works with large dataset as well\n",
    "  - Calls *custom data loader function (need to be written by ourself)* to load next chunk of data\n",
    "<br><br>\n",
    " 3. Setup a custom data pipeline\n",
    "  - Scales to infintely large dataset\n",
    "  - <font color=red> Requires writing TensorFlow specfic code</font>\n",
    "  - <font color=green> Supports parallel processing</font>\n",
    "<br><br>\n",
    "\n",
    "\n",
    "**Data Pipeline Example**\n",
    " - Tensorflow provides functions & helpers to build each stage of the pipeline\n",
    "   <img src=\"Images/Data Pipeline example.png\" width=\"500\" height=\"250\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jump: **3.1 load data final.py** for the complete program"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Story line \n",
    "\n",
    "Aim: \n",
    "- To design a neural network using tensor flow to predict the 'total_earnings' of video games based on 9 different features such as 'critic_rating', 'is_action',... which are stored in an excel file\n",
    "\n",
    "Step 1: **Data Preprocessing: Loading & cleaning data**\n",
    "\n",
    "Step 2: **Build a Neural network model**\n",
    "\n",
    "Step 3: **Train the network**\n",
    "\n",
    "Step 4: **Predict the output**\n",
    "\n",
    "Step 5: Tensorboard: Log the progress (optional)\n",
    "\n",
    "Step 6: Save model checkpoint (optional)\n",
    "\n",
    "Step 7: Load model checkpoint (optional)\n",
    "\n",
    "Step 8: Tensboard: Visualise the computational graph (optional)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Data Preprocessing: Loading & cleaning data**\n",
    "- At first, we have to read the .csv file and store it as a pandas dataframe.\n",
    "- Then, pullout the columns needed for X (input = data to train with) and Y (output = value to predict). We have to do it for both training and testing data\n",
    "- Then, scale the features/columns using built in scientific libraries such as `MinMaxSCaler` from `sklearn`. It's very important that the training and test data are scaled with the same scaler. Store the scaling parameters for future when we retrieve the results back fron NN prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load training data set from CSV file\n",
    "training_data_df = pd.read_csv('Exercise Files/03/sales_data_training.csv', dtype = float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 10)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display the number of rows and columns\n",
    "(training_data_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['critic_rating', 'is_action', 'is_exclusive_to_us', 'is_portable',\n",
       "       'is_role_playing', 'is_sequel', 'is_sports', 'suitable_for_kids',\n",
       "       'total_earnings', 'unit_price'], dtype=object)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display the header rows\n",
    "(training_data_df.columns.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>critic_rating</th>\n",
       "      <th>is_action</th>\n",
       "      <th>is_exclusive_to_us</th>\n",
       "      <th>is_portable</th>\n",
       "      <th>is_role_playing</th>\n",
       "      <th>is_sequel</th>\n",
       "      <th>is_sports</th>\n",
       "      <th>suitable_for_kids</th>\n",
       "      <th>total_earnings</th>\n",
       "      <th>unit_price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>132717.0</td>\n",
       "      <td>59.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>83407.0</td>\n",
       "      <td>49.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>62423.0</td>\n",
       "      <td>49.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>69889.0</td>\n",
       "      <td>39.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>161382.0</td>\n",
       "      <td>59.99</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   critic_rating  is_action  is_exclusive_to_us  is_portable  is_role_playing  \\\n",
       "0            3.5        1.0                 0.0          1.0              0.0   \n",
       "1            4.5        0.0                 0.0          0.0              0.0   \n",
       "2            3.0        0.0                 0.0          0.0              0.0   \n",
       "3            4.5        1.0                 0.0          0.0              0.0   \n",
       "4            4.0        1.0                 0.0          1.0              0.0   \n",
       "\n",
       "   is_sequel  is_sports  suitable_for_kids  total_earnings  unit_price  \n",
       "0        1.0        0.0                0.0        132717.0       59.99  \n",
       "1        1.0        1.0                0.0         83407.0       49.99  \n",
       "2        1.0        1.0                0.0         62423.0       49.99  \n",
       "3        0.0        0.0                1.0         69889.0       39.99  \n",
       "4        1.0        0.0                1.0        161382.0       59.99  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display the first few rows of the loaded data in a nicely formatted way\n",
    "training_data_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>critic_rating</th>\n",
       "      <th>is_action</th>\n",
       "      <th>is_exclusive_to_us</th>\n",
       "      <th>is_portable</th>\n",
       "      <th>is_role_playing</th>\n",
       "      <th>is_sequel</th>\n",
       "      <th>is_sports</th>\n",
       "      <th>suitable_for_kids</th>\n",
       "      <th>total_earnings</th>\n",
       "      <th>unit_price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.00000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>3.660000</td>\n",
       "      <td>0.466000</td>\n",
       "      <td>0.267000</td>\n",
       "      <td>0.24300</td>\n",
       "      <td>0.347000</td>\n",
       "      <td>0.746000</td>\n",
       "      <td>0.187000</td>\n",
       "      <td>0.270000</td>\n",
       "      <td>110705.229000</td>\n",
       "      <td>54.170000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.834024</td>\n",
       "      <td>0.499092</td>\n",
       "      <td>0.442614</td>\n",
       "      <td>0.42911</td>\n",
       "      <td>0.476254</td>\n",
       "      <td>0.435515</td>\n",
       "      <td>0.390107</td>\n",
       "      <td>0.444182</td>\n",
       "      <td>44970.558163</td>\n",
       "      <td>8.036927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>31355.000000</td>\n",
       "      <td>39.990000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>78830.250000</td>\n",
       "      <td>49.990000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>3.750000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>104335.500000</td>\n",
       "      <td>59.990000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>4.500000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>133271.000000</td>\n",
       "      <td>59.990000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>5.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>301860.000000</td>\n",
       "      <td>59.990000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       critic_rating    is_action  is_exclusive_to_us  is_portable  \\\n",
       "count    1000.000000  1000.000000         1000.000000   1000.00000   \n",
       "mean        3.660000     0.466000            0.267000      0.24300   \n",
       "std         0.834024     0.499092            0.442614      0.42911   \n",
       "min         2.000000     0.000000            0.000000      0.00000   \n",
       "25%         3.000000     0.000000            0.000000      0.00000   \n",
       "50%         3.750000     0.000000            0.000000      0.00000   \n",
       "75%         4.500000     1.000000            1.000000      0.00000   \n",
       "max         5.000000     1.000000            1.000000      1.00000   \n",
       "\n",
       "       is_role_playing    is_sequel    is_sports  suitable_for_kids  \\\n",
       "count      1000.000000  1000.000000  1000.000000        1000.000000   \n",
       "mean          0.347000     0.746000     0.187000           0.270000   \n",
       "std           0.476254     0.435515     0.390107           0.444182   \n",
       "min           0.000000     0.000000     0.000000           0.000000   \n",
       "25%           0.000000     0.000000     0.000000           0.000000   \n",
       "50%           0.000000     1.000000     0.000000           0.000000   \n",
       "75%           1.000000     1.000000     0.000000           1.000000   \n",
       "max           1.000000     1.000000     1.000000           1.000000   \n",
       "\n",
       "       total_earnings   unit_price  \n",
       "count     1000.000000  1000.000000  \n",
       "mean    110705.229000    54.170000  \n",
       "std      44970.558163     8.036927  \n",
       "min      31355.000000    39.990000  \n",
       "25%      78830.250000    49.990000  \n",
       "50%     104335.500000    59.990000  \n",
       "75%     133271.000000    59.990000  \n",
       "max     301860.000000    59.990000  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display some stats about the columns\n",
    "training_data_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>critic_rating</th>\n",
       "      <th>is_action</th>\n",
       "      <th>is_exclusive_to_us</th>\n",
       "      <th>is_portable</th>\n",
       "      <th>is_role_playing</th>\n",
       "      <th>is_sequel</th>\n",
       "      <th>is_sports</th>\n",
       "      <th>suitable_for_kids</th>\n",
       "      <th>unit_price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>59.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>49.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>49.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>39.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>59.99</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   critic_rating  is_action  is_exclusive_to_us  is_portable  is_role_playing  \\\n",
       "0            3.5        1.0                 0.0          1.0              0.0   \n",
       "1            4.5        0.0                 0.0          0.0              0.0   \n",
       "2            3.0        0.0                 0.0          0.0              0.0   \n",
       "3            4.5        1.0                 0.0          0.0              0.0   \n",
       "4            4.0        1.0                 0.0          1.0              0.0   \n",
       "\n",
       "   is_sequel  is_sports  suitable_for_kids  unit_price  \n",
       "0        1.0        0.0                0.0       59.99  \n",
       "1        1.0        1.0                0.0       49.99  \n",
       "2        1.0        1.0                0.0       49.99  \n",
       "3        0.0        0.0                1.0       39.99  \n",
       "4        1.0        0.0                1.0       59.99  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(1000, 9)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pull out columns for X (data to train with) and Y (value to predict)\n",
    "training_data_df.drop('total_earnings', axis=1).head()\n",
    "X_training = training_data_df.drop('total_earnings', axis=1).values\n",
    "X_training.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>total_earnings</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>132717.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>83407.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>62423.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>69889.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>161382.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   total_earnings\n",
       "0        132717.0\n",
       "1         83407.0\n",
       "2         62423.0\n",
       "3         69889.0\n",
       "4        161382.0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(1000, 1)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pull out columns for X (data to train with) and Y (value to predict)\n",
    "training_data_df[['total_earnings']].head()\n",
    "Y_training = training_data_df[['total_earnings']].values\n",
    "Y_training.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_testing (400, 9)\n",
      "Y_testing (400, 1)\n"
     ]
    }
   ],
   "source": [
    "# Load testing data set from CSV file\n",
    "test_data_df =pd.read_csv('Exercise Files/03/sales_data_test.csv', dtype = float)\n",
    "\n",
    "# Pull out columns for X (data to train with) and Y (value to predict)\n",
    "X_testing = test_data_df.drop('total_earnings', axis=1).values\n",
    "Y_testing = test_data_df[['total_earnings']].values\n",
    "\n",
    "print('X_testing', X_testing.shape)\n",
    "print('Y_testing', Y_testing.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All data needs to be scaled to a small range like 0 to 1 for the neural\n",
    "# network to work well. Create scalers for the inputs and outputs.\n",
    "X_scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "Y_scaler = MinMaxScaler(feature_range=(0, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'*************Before scaling the trained data set*************'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.00000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>3.660000</td>\n",
       "      <td>0.466000</td>\n",
       "      <td>0.267000</td>\n",
       "      <td>0.24300</td>\n",
       "      <td>0.347000</td>\n",
       "      <td>0.746000</td>\n",
       "      <td>0.187000</td>\n",
       "      <td>0.270000</td>\n",
       "      <td>54.170000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.834024</td>\n",
       "      <td>0.499092</td>\n",
       "      <td>0.442614</td>\n",
       "      <td>0.42911</td>\n",
       "      <td>0.476254</td>\n",
       "      <td>0.435515</td>\n",
       "      <td>0.390107</td>\n",
       "      <td>0.444182</td>\n",
       "      <td>8.036927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>39.990000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>49.990000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>3.750000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>59.990000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>4.500000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>59.990000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>5.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>59.990000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 0            1            2           3            4  \\\n",
       "count  1000.000000  1000.000000  1000.000000  1000.00000  1000.000000   \n",
       "mean      3.660000     0.466000     0.267000     0.24300     0.347000   \n",
       "std       0.834024     0.499092     0.442614     0.42911     0.476254   \n",
       "min       2.000000     0.000000     0.000000     0.00000     0.000000   \n",
       "25%       3.000000     0.000000     0.000000     0.00000     0.000000   \n",
       "50%       3.750000     0.000000     0.000000     0.00000     0.000000   \n",
       "75%       4.500000     1.000000     1.000000     0.00000     1.000000   \n",
       "max       5.000000     1.000000     1.000000     1.00000     1.000000   \n",
       "\n",
       "                 5            6            7            8  \n",
       "count  1000.000000  1000.000000  1000.000000  1000.000000  \n",
       "mean      0.746000     0.187000     0.270000    54.170000  \n",
       "std       0.435515     0.390107     0.444182     8.036927  \n",
       "min       0.000000     0.000000     0.000000    39.990000  \n",
       "25%       0.000000     0.000000     0.000000    49.990000  \n",
       "50%       1.000000     0.000000     0.000000    59.990000  \n",
       "75%       1.000000     0.000000     1.000000    59.990000  \n",
       "max       1.000000     1.000000     1.000000    59.990000  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "('*************Before scaling the trained data set*************')\n",
    "pd.DataFrame(X_training).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'*************After scaling the trained data set*************'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.00000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.553333</td>\n",
       "      <td>0.466000</td>\n",
       "      <td>0.267000</td>\n",
       "      <td>0.24300</td>\n",
       "      <td>0.347000</td>\n",
       "      <td>0.746000</td>\n",
       "      <td>0.187000</td>\n",
       "      <td>0.270000</td>\n",
       "      <td>0.709000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.278008</td>\n",
       "      <td>0.499092</td>\n",
       "      <td>0.442614</td>\n",
       "      <td>0.42911</td>\n",
       "      <td>0.476254</td>\n",
       "      <td>0.435515</td>\n",
       "      <td>0.390107</td>\n",
       "      <td>0.444182</td>\n",
       "      <td>0.401846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.583333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.833333</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 0            1            2           3            4  \\\n",
       "count  1000.000000  1000.000000  1000.000000  1000.00000  1000.000000   \n",
       "mean      0.553333     0.466000     0.267000     0.24300     0.347000   \n",
       "std       0.278008     0.499092     0.442614     0.42911     0.476254   \n",
       "min       0.000000     0.000000     0.000000     0.00000     0.000000   \n",
       "25%       0.333333     0.000000     0.000000     0.00000     0.000000   \n",
       "50%       0.583333     0.000000     0.000000     0.00000     0.000000   \n",
       "75%       0.833333     1.000000     1.000000     0.00000     1.000000   \n",
       "max       1.000000     1.000000     1.000000     1.00000     1.000000   \n",
       "\n",
       "                 5            6            7            8  \n",
       "count  1000.000000  1000.000000  1000.000000  1000.000000  \n",
       "mean      0.746000     0.187000     0.270000     0.709000  \n",
       "std       0.435515     0.390107     0.444182     0.401846  \n",
       "min       0.000000     0.000000     0.000000     0.000000  \n",
       "25%       0.000000     0.000000     0.000000     0.500000  \n",
       "50%       1.000000     0.000000     0.000000     1.000000  \n",
       "75%       1.000000     0.000000     1.000000     1.000000  \n",
       "max       1.000000     1.000000     1.000000     1.000000  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Scale both the training inputs and outputs: The min and max of all the columns will be in the feature_range(0,1) after scaling & std will be around ~0.5\n",
    "X_scaled_training = X_scaler.fit_transform(X_training)\n",
    "Y_scaled_training = Y_scaler.fit_transform(Y_training)\n",
    "('*************After scaling the trained data set*************')\n",
    "pd.DataFrame(X_scaled_training).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multiplication factor: 1.0 Addition factor: 0.0\n",
      "Multiplication factor: 3.69678933846e-06 Addition factor: -0.115912829707\n"
     ]
    }
   ],
   "source": [
    "print('Multiplication factor:',X_scaler.scale_[1], 'Addition factor:',X_scaler.min_[1])\n",
    "print('Multiplication factor:',Y_scaler.scale_[0], 'Addition factor:',Y_scaler.min_[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# It's very important that the training and test data are scaled with the same scaler.\n",
    "# So, instead of fit_transform, we use transform to scale the test data using the 'min' & 'max' values found in training data\n",
    "X_scaled_testing = X_scaler.transform(X_testing)\n",
    "Y_scaled_testing = Y_scaler.transform(Y_testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: X[0] values were scaled by multiplying by 0.3333333333 and adding -0.6667\n",
      "Note: Y[0] values were scaled by multiplying by 0.0000036968 and adding -0.1159\n"
     ]
    }
   ],
   "source": [
    "# We need to know the multiplication & addition factors to recover the original X from NN predictions later\n",
    "print(\"Note: X[0] values were scaled by multiplying by {:.10f} and adding {:.4f}\".format(X_scaler.scale_[0], X_scaler.min_[0]))\n",
    "print(\"Note: Y[0] values were scaled by multiplying by {:.10f} and adding {:.4f}\".format(Y_scaler.scale_[0], Y_scaler.min_[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Build a Neural network model**\n",
    "\n",
    "- 0. At first, we have to define the \n",
    "    - number of input nodes/ features, number of output nodes, \n",
    "    - number of neurons in each layer\n",
    "    - learning rate, training_epochs\n",
    "- 1. Next, we have to define the inputs & outputs needed for each layer & initialise the variables. Uses `xavier_initializer`. *By putting nodes inside the scope, tf makes diagrams/ computational graph that are more easier to understand as Everything within the same scope will be grouped together in the diagram*.\n",
    "    - Input layer\n",
    "        - `Placeholder: X `\n",
    "    - layer 1\n",
    "        - `Variables: weights1, biases1`\n",
    "        - `Input: X`\n",
    "        - `Output: layer_1_output`\n",
    "    - layer 2\n",
    "        - `Variables: weights2, biases2`\n",
    "        - `Input: layer_1_output`\n",
    "        - `Output: layer_2_output`\n",
    "    - layer 3\n",
    "        - `Variables: weights3, biases3`\n",
    "        - `Input: layer_2_output`\n",
    "        - `Output: layer_3_output`\n",
    "    - Output layer\n",
    "        - `Variables: weights4, biases4`\n",
    "        - `Input: layer_3_output`\n",
    "        - `Output: prediction`\n",
    "- 2. Define the cost function (mean squared error(prediction,Y)) of the neural network that will measure prediction accuracy during training\n",
    "    - cost \n",
    "        - `Placeholder: Y `         \n",
    "        - `Input: prediction`                   (True input: X)\n",
    "        - `Output: cost`\n",
    "- 3. Define the optimizer function that will be run to optimize the neural network. Uses `AdamOptimizer`\n",
    "    - train\n",
    "        -`Input: cost   `                      (True input: X,Y)\n",
    "        - `Output: optimizer`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define model parameters\n",
    "learning_rate = 0.001\n",
    "training_epochs = 100           # \n",
    "display_step = 5\n",
    "\n",
    "# Define how many input and output nodes are in our neural network\n",
    "number_of_inputs = 9\n",
    "number_of_outputs = 1\n",
    "\n",
    "# Define how many neurons we want in each layer of our neural network\n",
    "layer_1_nodes = 50\n",
    "layer_2_nodes = 100\n",
    "layer_3_nodes = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Section One: Define the layers of the neural network itself\n",
    "\n",
    "# Input Layer\n",
    "with tf.variable_scope('input'):\n",
    "    X = tf.placeholder(dtype=tf.float32, shape=(None,number_of_inputs))\n",
    "                                                # 'None' allows to accept batches of any size of inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Layer 1\n",
    "with tf.variable_scope('layer_1'):\n",
    "    weights = tf.get_variable(name = 'weights1', shape = (number_of_inputs, layer_1_nodes), initializer=tf.contrib.layers.xavier_initializer())\n",
    "    biases = tf.get_variable(name = 'biases1', shape = (layer_1_nodes), initializer=tf.zeros_initializer())\n",
    "    layer_1_output = tf.nn.relu(tf.matmul(X,weights) + biases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Layer 2\n",
    "with tf.variable_scope('layer_2'):\n",
    "    weights = tf.get_variable(name = 'weights2', shape = (layer_1_nodes, layer_2_nodes), initializer=tf.contrib.layers.xavier_initializer())\n",
    "    biases = tf.get_variable(name = 'biases2', shape = (layer_2_nodes), initializer=tf.zeros_initializer())\n",
    "    layer_2_output = tf.nn.relu(tf.matmul(layer_1_output,weights) + biases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Layer 3\n",
    "with tf.variable_scope('layer_3'):\n",
    "    weights = tf.get_variable(name = 'weights3', shape = (layer_2_nodes, layer_3_nodes), initializer=tf.contrib.layers.xavier_initializer())\n",
    "    biases = tf.get_variable(name = 'biases3', shape = (layer_3_nodes), initializer=tf.zeros_initializer())\n",
    "    layer_3_output = tf.nn.relu(tf.matmul(layer_2_output,weights) + biases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Output Layer\n",
    "with tf.variable_scope('output'):\n",
    "    weights = tf.get_variable(name = 'weights4', shape = (layer_3_nodes, number_of_outputs), initializer=tf.contrib.layers.xavier_initializer())\n",
    "    biases = tf.get_variable(name = 'biases4', shape = (number_of_outputs), initializer=tf.zeros_initializer())\n",
    "    prediction = tf.nn.relu(tf.matmul(layer_3_output,weights) + biases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Section Two: Define the cost function of the neural network that will measure prediction accuracy during training\n",
    "\n",
    "with tf.variable_scope('cost'):\n",
    "    Y = tf.placeholder(dtype=tf.float32, shape=(None,number_of_outputs))\n",
    "    cost = tf.reduce_mean(tf.squared_difference(Y,prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section Three: Define the optimizer function that will be run to optimize the neural network\n",
    "\n",
    "with tf.variable_scope('train'):\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate).minimize(cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Train the network**\n",
    "\n",
    "- Initialise the tensorflow session and then initialise all the variables & layers of NN\n",
    "- Run the optmiser over & over to train the newtork\n",
    "    - 100 epoch -> 100 iterations over the entire training dataset to train the Neural Network\n",
    "    - Every 'x' training steps, prin the progress including training_cost & tesing_cost \n",
    "- Display the final training cost & testing cost "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training is started!\n",
      "\n",
      "epoch, training_cost, testing_cost\n",
      "0 0.111389 0.119822\n",
      "5 0.0353743 0.0391504\n",
      "10 0.0224205 0.0237956\n",
      "15 0.0147202 0.0153046\n",
      "20 0.00600003 0.00625604\n",
      "25 0.00445873 0.00439445\n",
      "30 0.00349639 0.00387993\n",
      "35 0.00196094 0.00189238\n",
      "40 0.00134309 0.00132727\n",
      "45 0.00123401 0.00124294\n",
      "50 0.00104241 0.0010441\n",
      "55 0.000858376 0.000962575\n",
      "60 0.000712194 0.000788833\n",
      "65 0.000584863 0.000714467\n",
      "70 0.000495589 0.000599044\n",
      "75 0.000428674 0.000560759\n",
      "80 0.000378921 0.000503913\n",
      "85 0.000335462 0.000472346\n",
      "90 0.00029717 0.000432161\n",
      "95 0.000265844 0.000411471\n",
      "\n",
      "\n",
      "Training is complete!\n",
      "Final Training Cost: 0.00024512\n",
      "Final Testing Cost 0.000388652\n"
     ]
    }
   ],
   "source": [
    "# Initialize a session so that we can run TensorFlow operations on the Graph\n",
    "session = tf.Session()\n",
    "\n",
    "# Run the global variable initializer to initialize all variables and layers of the neural network\n",
    "session.run(tf.global_variables_initializer())\n",
    "\n",
    "# Run the optimizer over and over to train the network.\n",
    "# One epoch is one full run through the training data set.\n",
    "\n",
    "print(\"\\n\\nTraining is started!\\n\")\n",
    "print ('epoch, training_cost, testing_cost')\n",
    "for epoch in range(training_epochs):\n",
    "\n",
    "    # Feed in the training data and do one step of neural network training\n",
    "    session.run(optimizer, feed_dict={X:X_scaled_training, Y:Y_scaled_training})\n",
    "\n",
    "    # Print the current training status to the screen\n",
    "    #print(\"Training pass: {}\".format(epoch))\n",
    "\n",
    "    # Every 5 training steps, log our progress\n",
    "    if epoch%5 == 0:\n",
    "        training_cost = session.run(cost, feed_dict={X:X_scaled_training, Y:Y_scaled_training})\n",
    "        testing_cost = session.run(cost, feed_dict={X:X_scaled_testing, Y:Y_scaled_testing})\n",
    "        print (epoch, training_cost, testing_cost)\n",
    "\n",
    "# Training is now complete!\n",
    "print(\"\\n\\nTraining is complete!\")\n",
    "\n",
    "final_training_cost = session.run(cost, feed_dict={X:X_scaled_training, Y:Y_scaled_training})\n",
    "final_testing_cost = session.run(cost, feed_dict={X:X_scaled_testing, Y:Y_scaled_testing})\n",
    "print (\"Final Training Cost:\",final_training_cost)\n",
    "print (\"Final Testing Cost\", final_testing_cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " #### **Predict the output**\n",
    "  - Run the `prediction` to get `Y_prediction_scaled` for the test data set (new data that the model never seen before)\n",
    "  - Unscale the predicted output back to original form\n",
    "  - Then, print some of them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Game #id</th>\n",
       "      <th>real_earnings $</th>\n",
       "      <th>predicted_earnings $</th>\n",
       "      <th>difference $</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>272</td>\n",
       "      <td>246429.0</td>\n",
       "      <td>239592.093750</td>\n",
       "      <td>6836.906250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>118</td>\n",
       "      <td>125590.0</td>\n",
       "      <td>128092.523438</td>\n",
       "      <td>2502.523438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>281</td>\n",
       "      <td>89874.0</td>\n",
       "      <td>89793.320312</td>\n",
       "      <td>80.679688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>34</td>\n",
       "      <td>82558.0</td>\n",
       "      <td>89417.023438</td>\n",
       "      <td>6859.023438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>270</td>\n",
       "      <td>92117.0</td>\n",
       "      <td>90848.250000</td>\n",
       "      <td>1268.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>3</td>\n",
       "      <td>137456.0</td>\n",
       "      <td>139637.359375</td>\n",
       "      <td>2181.359375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>358</td>\n",
       "      <td>95340.0</td>\n",
       "      <td>94863.273438</td>\n",
       "      <td>476.726562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>101</td>\n",
       "      <td>127332.0</td>\n",
       "      <td>128092.523438</td>\n",
       "      <td>760.523438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>103</td>\n",
       "      <td>86947.0</td>\n",
       "      <td>84467.718750</td>\n",
       "      <td>2479.281250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>259</td>\n",
       "      <td>111063.0</td>\n",
       "      <td>117850.039062</td>\n",
       "      <td>6787.039062</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Game #id  real_earnings $  predicted_earnings $  difference $\n",
       "0       272         246429.0         239592.093750   6836.906250\n",
       "1       118         125590.0         128092.523438   2502.523438\n",
       "2       281          89874.0          89793.320312     80.679688\n",
       "3        34          82558.0          89417.023438   6859.023438\n",
       "4       270          92117.0          90848.250000   1268.750000\n",
       "5         3         137456.0         139637.359375   2181.359375\n",
       "6       358          95340.0          94863.273438    476.726562\n",
       "7       101         127332.0         128092.523438    760.523438\n",
       "8       103          86947.0          84467.718750   2479.281250\n",
       "9       259         111063.0         117850.039062   6787.039062"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now that the neural network is trained, let's use it to make predictions for our test data.\n",
    "# Pass in the X testing data and run the \"prediciton\" operation\n",
    "Y_predicted_scaled = session.run(prediction, feed_dict={X: X_scaled_testing})\n",
    "\n",
    "# Unscale the data back to it's original units (dollars)\n",
    "Y_predicted = Y_scaler.inverse_transform(Y_predicted_scaled)\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "# randomly select few testing data to predict\n",
    "l = list()\n",
    "for i in random.sample(range(1, Y_predicted.shape[0]), 10):\n",
    "    real_earnings = test_data_df['total_earnings'].values[i]\n",
    "    predicted_earnings = Y_predicted[i][0]\n",
    "\n",
    "    #print(\"The actual earnings of Game #{:>3} were    ${}\".format(i,real_earnings))\n",
    "    #print(\"Our neural network predicted earnings of ${}\\n\".format(predicted_earnings))\n",
    "    \n",
    "    l.append([i,real_earnings, predicted_earnings, abs(real_earnings-predicted_earnings)])\n",
    "    \n",
    "prediction_df = pd.DataFrame(l, columns=['Game #id', 'real_earnings $', 'predicted_earnings $', 'difference $'])\n",
    "\n",
    "prediction_df "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Log the progress\n",
    "- Create a summary operation to log network progress (E.g. `cost` which is a scalar quantity)\n",
    "- After session initialisation, Create log file writers to record the progress. Seperate writers for\n",
    "    - training\n",
    "    - testing\n",
    "- During the training iteration, \n",
    "    - Run the summary session and\n",
    "    - Write the current training status to the log files (which we can view using the TensorBoard) \n",
    "- *To open tensorboard*, refer section **4.2 model logging final.py**\n",
    "\n",
    "- There is a disadvantage in current logging. For better loggin, refer section `5.2 visualize_training.py` which adds each log file into a new different folder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Remember to run the tensorboard with folder 04*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'logging/current_cost:0' shape=() dtype=string>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training is started!\n",
      "Epoch: 0 - Training Cost: 0.09700524806976318,  Testing Cost: 0.10383152216672897\n",
      "Epoch: 5 - Training Cost: 0.03186769410967827,  Testing Cost: 0.034066542983055115\n",
      "Epoch: 10 - Training Cost: 0.02242102287709713,  Testing Cost: 0.023995129391551018\n",
      "Epoch: 15 - Training Cost: 0.01460286695510149,  Testing Cost: 0.015454815700650215\n",
      "Epoch: 20 - Training Cost: 0.010955827310681343,  Testing Cost: 0.011591535992920399\n",
      "Epoch: 25 - Training Cost: 0.007398528978228569,  Testing Cost: 0.0077186524868011475\n",
      "Epoch: 30 - Training Cost: 0.005617290269583464,  Testing Cost: 0.005816243123263121\n",
      "Epoch: 35 - Training Cost: 0.004631402902305126,  Testing Cost: 0.004925246816128492\n",
      "Epoch: 40 - Training Cost: 0.003131263190880418,  Testing Cost: 0.0032598667312413454\n",
      "Epoch: 45 - Training Cost: 0.002337668789550662,  Testing Cost: 0.002428404288366437\n",
      "Epoch: 50 - Training Cost: 0.0018506181659176946,  Testing Cost: 0.0019455732544884086\n",
      "Epoch: 55 - Training Cost: 0.0014199267607182264,  Testing Cost: 0.0014851597370579839\n",
      "Epoch: 60 - Training Cost: 0.0011091383639723063,  Testing Cost: 0.0011971453204751015\n",
      "Epoch: 65 - Training Cost: 0.0009010278154164553,  Testing Cost: 0.0009407074539922178\n",
      "Epoch: 70 - Training Cost: 0.0007430528639815748,  Testing Cost: 0.0007891946006566286\n",
      "Epoch: 75 - Training Cost: 0.0006142726051621139,  Testing Cost: 0.0006624002708122134\n",
      "Epoch: 80 - Training Cost: 0.0005188021459616721,  Testing Cost: 0.0005868478328920901\n",
      "Epoch: 85 - Training Cost: 0.0004478777991607785,  Testing Cost: 0.00050813501002267\n",
      "Epoch: 90 - Training Cost: 0.00038914484321139753,  Testing Cost: 0.0004474854504223913\n",
      "Epoch: 95 - Training Cost: 0.0003375878732185811,  Testing Cost: 0.0003974726132582873\n",
      "\n",
      "\n",
      "Training is complete!\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Attempted to use a closed Session.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-24-09e8a39b2bd9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     44\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m \u001b[1;31m# Get the final accuracy scores by running the \"cost\" operation on the training and test data sets\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 46\u001b[1;33m \u001b[0mfinal_training_cost\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcost\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mX_scaled_training\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mY_scaled_training\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     47\u001b[0m \u001b[0mfinal_testing_cost\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcost\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mX_scaled_testing\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mY_scaled_testing\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[0mprint\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m\"Final Training Cost:\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mfinal_training_cost\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\prasanth\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    887\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    888\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 889\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    890\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    891\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\prasanth\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1045\u001b[0m     \u001b[1;31m# Check session.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1046\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_closed\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1047\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Attempted to use a closed Session.'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1048\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mversion\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1049\u001b[0m       raise RuntimeError('The Session graph is empty.  Add operations to the '\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Attempted to use a closed Session."
     ]
    }
   ],
   "source": [
    "# Create a summary operation to log the progress of the network\n",
    "with tf.variable_scope('logging'):\n",
    "    tf.summary.scalar('current_cost', cost)\n",
    "    summary = tf.summary.merge_all()\n",
    "\n",
    "# Initialize a session so that we can run TensorFlow operations on the Graph\n",
    "with tf.Session() as session:\n",
    "\n",
    "    # Run the global variable initializer to initialize all variables and layers of the neural network\n",
    "    session.run(tf.global_variables_initializer())\n",
    "\n",
    "    # Create log file writers to record training progress.\n",
    "    # We'll store training and testing log data separately.\n",
    "    training_writer = tf.summary.FileWriter(\"./Exercise Files/04/logs/training\", session.graph)\n",
    "    testing_writer = tf.summary.FileWriter(\"./Exercise Files/04/logs/testing\", session.graph)\n",
    "\n",
    "    # Run the optimizer over and over to train the network.\n",
    "    # One epoch is one full run through the training data set.\n",
    "    print(\"\\n\\nTraining is started!\")\n",
    "\n",
    "    for epoch in range(training_epochs):\n",
    "\n",
    "        # Feed in the training data and do one step of neural network training\n",
    "        session.run(optimizer, feed_dict={X:X_scaled_training, Y:Y_scaled_training})\n",
    "\n",
    "        # Print the current training status to the screen\n",
    "        #print(\"Training pass: {}\".format(epoch))\n",
    "\n",
    "        # Every 5 training steps, log our progress\n",
    "        if epoch%5 == 0:\n",
    "            # Get the current accuracy scores by running the \"cost\" operation on the training and test data sets\n",
    "            training_cost, training_summary = session.run([cost, summary], feed_dict={X:X_scaled_training, Y:Y_scaled_training})\n",
    "            testing_cost, testing_summary = session.run([cost, summary], feed_dict={X:X_scaled_testing, Y:Y_scaled_testing})\n",
    "\n",
    "            # Write the current training status to the log files (Which we can view with TensorBoard)\n",
    "            training_writer.add_summary(training_summary, epoch)   # Here, epoch is x axis & training_summary is y axis\n",
    "            testing_writer.add_summary(testing_summary, epoch)\n",
    "\n",
    "            print(\"Epoch: {} - Training Cost: {},  Testing Cost: {}\".format(epoch, training_cost, testing_cost))\n",
    "\n",
    "\n",
    "# Training is now complete!\n",
    "print(\"\\n\\nTraining is complete!\")\n",
    "\n",
    "# Get the final accuracy scores by running the \"cost\" operation on the training and test data sets\n",
    "final_training_cost = session.run(cost, feed_dict={X:X_scaled_training, Y:Y_scaled_training})\n",
    "final_testing_cost = session.run(cost, feed_dict={X:X_scaled_testing, Y:Y_scaled_testing})\n",
    "print (\"Final Training Cost:\",final_training_cost)\n",
    "print (\"Final Testing Cost\", final_testing_cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### save model checkpoint\n",
    "Useful when we don't want to train the model everytime or port over the trained model to other sytems. **Note that training a model is usually very time & resource consuming**\n",
    "- Before session initialization, create a `saver` object\n",
    "- After the completing of training, save the checkpoint by giving it a name. *Remember the file location and name*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training is started!\n",
      "\n",
      "epoch, training_cost, testing_cost\n",
      "0 0.0756341 0.0827737\n",
      "5 0.023247 0.0256765\n",
      "10 0.0174276 0.0184586\n",
      "15 0.008233 0.00904954\n",
      "20 0.00735354 0.00793681\n",
      "25 0.00423942 0.00405371\n",
      "30 0.00347225 0.00326526\n",
      "35 0.00289848 0.00293897\n",
      "40 0.00198947 0.00192861\n",
      "45 0.00159938 0.00154612\n",
      "50 0.00123217 0.00125773\n",
      "55 0.000921636 0.000999829\n",
      "60 0.00072547 0.000818369\n",
      "65 0.000567142 0.0006509\n",
      "70 0.000466626 0.000559298\n",
      "75 0.000386177 0.000478366\n",
      "80 0.00032895 0.000425821\n",
      "85 0.000282265 0.000365213\n",
      "90 0.000244879 0.000323456\n",
      "95 0.000215621 0.000287698\n",
      "\n",
      "\n",
      "Training is complete!\n",
      "Final Training Cost: 0.000196278\n",
      "Final Testing Cost 0.000268464\n"
     ]
    }
   ],
   "source": [
    "# Initialize a session so that we can run TensorFlow operations on the Graph\n",
    "session = tf.Session()\n",
    "\n",
    "# Run the global variable initializer to initialize all variables and layers of the neural network\n",
    "session.run(tf.global_variables_initializer())\n",
    "\n",
    "# Run the optimizer over and over to train the network.\n",
    "# One epoch is one full run through the training data set.\n",
    "\n",
    "print(\"\\n\\nTraining is started!\\n\")\n",
    "print ('epoch, training_cost, testing_cost')\n",
    "for epoch in range(training_epochs):\n",
    "\n",
    "    # Feed in the training data and do one step of neural network training\n",
    "    session.run(optimizer, feed_dict={X:X_scaled_training, Y:Y_scaled_training})\n",
    "\n",
    "    # Print the current training status to the screen\n",
    "    #print(\"Training pass: {}\".format(epoch))\n",
    "\n",
    "    # Every 5 training steps, log our progress\n",
    "    if epoch%5 == 0:\n",
    "        training_cost = session.run(cost, feed_dict={X:X_scaled_training, Y:Y_scaled_training})\n",
    "        testing_cost = session.run(cost, feed_dict={X:X_scaled_testing, Y:Y_scaled_testing})\n",
    "        print (epoch, training_cost, testing_cost)\n",
    "\n",
    "# Training is now complete!\n",
    "print(\"\\n\\nTraining is complete!\")\n",
    "\n",
    "final_training_cost = session.run(cost, feed_dict={X:X_scaled_training, Y:Y_scaled_training})\n",
    "final_testing_cost = session.run(cost, feed_dict={X:X_scaled_testing, Y:Y_scaled_testing})\n",
    "print (\"Final Training Cost:\",final_training_cost)\n",
    "print (\"Final Testing Cost\", final_testing_cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved: ./Exercise Files/04/logs/trained_model.ckpt\n"
     ]
    }
   ],
   "source": [
    "try: \n",
    "    save_path = saver.save(session, \"./Exercise Files/04/logs/trained_model.ckpt\")\n",
    "    print(\"Model saved: {}\".format(save_path))\n",
    "except:  \n",
    "    print ('Delete the existing checkpoints')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### load model checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Initialize a session so that we can run TensorFlow operations on the Graph\n",
    "with tf.Session() as session:\n",
    "\n",
    "    # Instead, load the pretrained network from disk:\n",
    "    saver.restore(session, \"./Exercise Files/04/logs/trained_model.ckpt\")\n",
    "    \n",
    "    \n",
    "    ##### NO TRAINING LOOP AS THE TRAINED MODEL IS ALREADY RESTORED FROM THE SAVED CHECKPOINT####\n",
    "    '''\n",
    "    # Run the global variable initializer to initialize all variables and layers of the neural network\n",
    "    # session.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Create log file writers to record training progress.\n",
    "    # We'll store training and testing log data separately.\n",
    "    training_writer = tf.summary.FileWriter(\"./Exercise Files/04/logs/training\", session.graph)\n",
    "    testing_writer = tf.summary.FileWriter(\"./Exercise Files/04/logs/testing\", session.graph)\n",
    "\n",
    "    # Run the optimizer over and over to train the network.\n",
    "    # One epoch is one full run through the training data set.\n",
    "    print(\"\\n\\nTraining is started!\")\n",
    "    \n",
    "    for epoch in range(training_epochs):\n",
    "        \n",
    "        # Feed in the training data and do one step of neural network training\n",
    "        session.run(optimizer, feed_dict={X:X_scaled_training, Y:Y_scaled_training})\n",
    "\n",
    "        # Print the current training status to the screen\n",
    "        #print(\"Training pass: {}\".format(epoch))\n",
    "        \n",
    "        # Every 5 training steps, log our progress\n",
    "        if epoch%5 == 0:\n",
    "            # Get the current accuracy scores by running the \"cost\" operation on the training and test data sets\n",
    "            training_cost, training_summary = session.run([cost, summary], feed_dict={X:X_scaled_training, Y:Y_scaled_training})\n",
    "            testing_cost, testing_summary = session.run([cost, summary], feed_dict={X:X_scaled_testing, Y:Y_scaled_testing})\n",
    "            \n",
    "            # Write the current training status to the log files (Which we can view with TensorBoard)\n",
    "            training_writer.add_summary(training_summary, epoch)   # Here, epoch is x axis & training_summary is y axis\n",
    "            testing_writer.add_summary(testing_summary, epoch)\n",
    "            \n",
    "            print(\"Epoch: {} - Training Cost: {},  Testing Cost: {}\".format(epoch, training_cost, testing_cost))\n",
    "\n",
    "            \n",
    "    # Training is now complete!\n",
    "    print(\"\\n\\nTraining is complete!\")\n",
    "    \n",
    "    '''\n",
    "    # Get the final accuracy scores by running the \"cost\" operation on the training and test data sets\n",
    "    final_training_cost = session.run(cost, feed_dict={X:X_scaled_training, Y:Y_scaled_training})\n",
    "    final_testing_cost = session.run(cost, feed_dict={X:X_scaled_testing, Y:Y_scaled_testing})\n",
    "    print (\"Final Training Cost:\",final_training_cost)\n",
    "    print (\"Final Testing Cost\", final_testing_cost)\n",
    "    \n",
    "    \n",
    "    # Now that the neural network is trained, let's use it to make predictions for our test data.\n",
    "    # Pass in the X testing data and run the \"prediciton\" operation\n",
    "    Y_predicted_scaled = session.run(prediction, feed_dict={X: X_scaled_testing})\n",
    "\n",
    "    # Unscale the data back to it's original units (dollars)\n",
    "    Y_predicted = Y_scaler.inverse_transform(Y_predicted_scaled)\n",
    "\n",
    "    real_earnings = test_data_df['total_earnings'].values[0]\n",
    "    predicted_earnings = Y_predicted[0][0]\n",
    "\n",
    "    print(\"The actual earnings of Game #1 were ${}\".format(real_earnings))\n",
    "    print(\"Our neural network predicted earnings of ${}\".format(predicted_earnings))\n",
    "    \n",
    "    save_path = saver.save(session, \"./Exercise Files/04/logs/trained_model.ckpt\")\n",
    "    print(\"Model saved: {}\".format(save_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tensorboard: Visualise the Computational Graph\n",
    "\n",
    "*Remember to run the tensorboard with folder `05`*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For further details, refere section **5.1.1  Visualise the Computational Graph**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_training (1000, 9)\n",
      "Y_training (1000, 1)\n",
      "X_testing (400, 9)\n",
      "Y_testing (400, 1)\n",
      "Note: X[0] values were scaled by multiplying by 0.3333333333 and adding -0.6667\n",
      "Note: Y[0] values were scaled by multiplying by 0.0000036968 and adding -0.1159\n",
      "\n",
      "\n",
      "Training is started!\n",
      "Epoch: 0 - Training Cost: 0.016792668029665947,  Testing Cost: 0.019449938088655472\n",
      "Epoch: 5 - Training Cost: 0.0060439337976276875,  Testing Cost: 0.0077941459603607655\n",
      "Epoch: 10 - Training Cost: 0.0035044257529079914,  Testing Cost: 0.004326626658439636\n",
      "Epoch: 15 - Training Cost: 0.001993266399949789,  Testing Cost: 0.002199414186179638\n",
      "Epoch: 20 - Training Cost: 0.0014342345530167222,  Testing Cost: 0.0013771180529147387\n",
      "Epoch: 25 - Training Cost: 0.0010704786982387304,  Testing Cost: 0.0010210477048531175\n",
      "Epoch: 30 - Training Cost: 0.0008153648232109845,  Testing Cost: 0.0007836938020773232\n",
      "Epoch: 35 - Training Cost: 0.0006313313497230411,  Testing Cost: 0.0006075658020563424\n",
      "Epoch: 40 - Training Cost: 0.0004928256385028362,  Testing Cost: 0.00043606021790765226\n",
      "Epoch: 45 - Training Cost: 0.00037346367025747895,  Testing Cost: 0.0003503448097035289\n",
      "Epoch: 50 - Training Cost: 0.000297009275527671,  Testing Cost: 0.00027908076299354434\n",
      "Epoch: 55 - Training Cost: 0.00024143674818333238,  Testing Cost: 0.00024253930314444005\n",
      "Epoch: 60 - Training Cost: 0.00020769164257217199,  Testing Cost: 0.00022749741037841886\n",
      "Epoch: 65 - Training Cost: 0.00017839430074673146,  Testing Cost: 0.00020378739282023162\n",
      "Epoch: 70 - Training Cost: 0.0001535246119601652,  Testing Cost: 0.0001871290005510673\n",
      "Epoch: 75 - Training Cost: 0.00013642071280628443,  Testing Cost: 0.0001755013654474169\n",
      "Epoch: 80 - Training Cost: 0.0001218123288708739,  Testing Cost: 0.00016250692715402693\n",
      "Epoch: 85 - Training Cost: 0.00010964865941787139,  Testing Cost: 0.00015186051314231008\n",
      "Epoch: 90 - Training Cost: 0.00010001126065617427,  Testing Cost: 0.00014597321569453925\n",
      "Epoch: 95 - Training Cost: 9.121209586737677e-05,  Testing Cost: 0.0001401267945766449\n",
      "\n",
      "\n",
      "Training is complete!\n",
      "Final Training Cost: 8.52984e-05\n",
      "Final Testing Cost 0.000135008\n",
      "The actual earnings of Game #1 were $247537.0\n",
      "Our neural network predicted earnings of $259631.109375\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Load training data set from CSV file\n",
    "training_data_df = pd.read_csv('Exercise Files/05/sales_data_training.csv', dtype = float)\n",
    "\n",
    "# Pull out columns for X (data to train with) and Y (value to predict)\n",
    "X_training = training_data_df.drop('total_earnings', axis=1).values\n",
    "Y_training = training_data_df[['total_earnings']].values\n",
    "\n",
    "print('X_training', X_training.shape)\n",
    "print('Y_training', Y_training.shape)\n",
    "\n",
    "# Load testing data set from CSV file\n",
    "test_data_df =pd.read_csv('Exercise Files/05/sales_data_test.csv', dtype = float)\n",
    "\n",
    "# Pull out columns for X (data to train with) and Y (value to predict)\n",
    "X_testing = test_data_df.drop('total_earnings', axis=1).values\n",
    "Y_testing = test_data_df[['total_earnings']].values\n",
    "\n",
    "print('X_testing', X_testing.shape)\n",
    "print('Y_testing', Y_testing.shape)\n",
    "\n",
    "# All data needs to be scaled to a small range like 0 to 1 for the neural\n",
    "# network to work well. Create scalers for the inputs and outputs.\n",
    "X_scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "Y_scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "\n",
    "# Scale both the training inputs and outputs\n",
    "X_scaled_training = X_scaler.fit_transform(X_training)\n",
    "Y_scaled_training = Y_scaler.fit_transform(Y_training)\n",
    "\n",
    "# It's very important that the training and test data are scaled with the same scaler.\n",
    "# So, instead of fit_transform, we use transform to scale the test data using the 'min' & 'max' values found in training data\n",
    "X_scaled_testing = X_scaler.transform(X_testing)\n",
    "Y_scaled_testing = Y_scaler.transform(Y_testing)\n",
    "\n",
    "# We need to know the multiplication & addition factors to recover the original X from NN predictions later\n",
    "print(\"Note: X[0] values were scaled by multiplying by {:.10f} and adding {:.4f}\".format(X_scaler.scale_[0], X_scaler.min_[0]))\n",
    "print(\"Note: Y[0] values were scaled by multiplying by {:.10f} and adding {:.4f}\".format(Y_scaler.scale_[0], Y_scaler.min_[0]))\n",
    "\n",
    "# Define model parameters\n",
    "learning_rate = 0.001\n",
    "training_epochs = 100\n",
    "display_step = 5\n",
    "\n",
    "# Define how many input and output nodes are in our neural network\n",
    "number_of_inputs = 9\n",
    "number_of_outputs = 1\n",
    "\n",
    "# Define how many neurons we want in each layer of our neural network\n",
    "layer_1_nodes = 50\n",
    "layer_2_nodes = 100\n",
    "layer_3_nodes = 50\n",
    "\n",
    "# Section One: Define the layers of the neural network itself\n",
    "\n",
    "# Input Layer\n",
    "with tf.variable_scope('input'):\n",
    "    X = tf.placeholder(dtype=tf.float32, shape=(None,number_of_inputs))\n",
    "                                                # 'None' allows to accept batches of any size of inputs\n",
    "# Layer 1\n",
    "with tf.variable_scope('layer_1'):\n",
    "    weights = tf.get_variable(name = 'weights1', shape = (number_of_inputs, layer_1_nodes), initializer=tf.contrib.layers.xavier_initializer())\n",
    "    biases = tf.get_variable(name = 'biases1', shape = (layer_1_nodes), initializer=tf.zeros_initializer())\n",
    "    layer_1_output = tf.nn.relu(tf.matmul(X,weights) + biases)\n",
    "\n",
    "# Layer 2\n",
    "with tf.variable_scope('layer_2'):\n",
    "    weights = tf.get_variable(name = 'weights2', shape = (layer_1_nodes, layer_2_nodes), initializer=tf.contrib.layers.xavier_initializer())\n",
    "    biases = tf.get_variable(name = 'biases2', shape = (layer_2_nodes), initializer=tf.zeros_initializer())\n",
    "    layer_2_output = tf.nn.relu(tf.matmul(layer_1_output,weights) + biases)\n",
    "\n",
    "# Layer 3\n",
    "with tf.variable_scope('layer_3'):\n",
    "    weights = tf.get_variable(name = 'weights3', shape = (layer_2_nodes, layer_3_nodes), initializer=tf.contrib.layers.xavier_initializer())\n",
    "    biases = tf.get_variable(name = 'biases3', shape = (layer_3_nodes), initializer=tf.zeros_initializer())\n",
    "    layer_3_output = tf.nn.relu(tf.matmul(layer_2_output,weights) + biases)\n",
    "\n",
    "# Output Layer\n",
    "with tf.variable_scope('output'):\n",
    "    weights = tf.get_variable(name = 'weights4', shape = (layer_3_nodes, number_of_outputs), initializer=tf.contrib.layers.xavier_initializer())\n",
    "    biases = tf.get_variable(name = 'biases4', shape = (number_of_outputs), initializer=tf.zeros_initializer())\n",
    "    prediction = tf.nn.relu(tf.matmul(layer_3_output,weights) + biases)\n",
    "\n",
    "\n",
    "# Section Two: Define the cost function of the neural network that will measure prediction accuracy during training\n",
    "\n",
    "with tf.variable_scope('cost'):\n",
    "    Y = tf.placeholder(dtype=tf.float32, shape=(None,number_of_outputs))\n",
    "    cost = tf.reduce_mean(tf.squared_difference(Y,prediction))\n",
    "\n",
    "\n",
    "# Section Three: Define the optimizer function that will be run to optimize the neural network\n",
    "\n",
    "with tf.variable_scope('train'):\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate).minimize(cost)\n",
    "\n",
    "# Create a summary operation to log the progress of the network\n",
    "with tf.variable_scope('logging'):\n",
    "    tf.summary.scalar('current_cost', cost)\n",
    "    summary = tf.summary.merge_all()\n",
    "\n",
    "# Initialize a session so that we can run TensorFlow operations on the Graph\n",
    "with tf.Session() as session:\n",
    "\n",
    "    # Run the global variable initializer to initialize all variables and layers of the neural network\n",
    "    session.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Create log file writers to record training progress.\n",
    "    # We'll store training and testing log data separately.\n",
    "    training_writer = tf.summary.FileWriter(\"./Exercise Files/05/logs/training\", session.graph)\n",
    "    testing_writer = tf.summary.FileWriter(\"./Exercise Files/05/logs/testing\", session.graph)\n",
    "\n",
    "    # Run the optimizer over and over to train the network.\n",
    "    # One epoch is one full run through the training data set.\n",
    "    print(\"\\n\\nTraining is started!\")\n",
    "    \n",
    "    for epoch in range(training_epochs):\n",
    "        \n",
    "        # Feed in the training data and do one step of neural network training\n",
    "        session.run(optimizer, feed_dict={X:X_scaled_training, Y:Y_scaled_training})\n",
    "\n",
    "        # Print the current training status to the screen\n",
    "        #print(\"Training pass: {}\".format(epoch))\n",
    "        \n",
    "        # Every 5 training steps, log our progress\n",
    "        if epoch%5 == 0:\n",
    "            # Get the current accuracy scores by running the \"cost\" operation on the training and test data sets\n",
    "            training_cost, training_summary = session.run([cost, summary], feed_dict={X:X_scaled_training, Y:Y_scaled_training})\n",
    "            testing_cost, testing_summary = session.run([cost, summary], feed_dict={X:X_scaled_testing, Y:Y_scaled_testing})\n",
    "            \n",
    "            # Write the current training status to the log files (Which we can view with TensorBoard)\n",
    "            training_writer.add_summary(training_summary, epoch)   # Here, epoch is x axis & training_summary is y axis\n",
    "            testing_writer.add_summary(testing_summary, epoch)\n",
    "            \n",
    "            print(\"Epoch: {} - Training Cost: {},  Testing Cost: {}\".format(epoch, training_cost, testing_cost))\n",
    "\n",
    "            \n",
    "    # Training is now complete!\n",
    "    print(\"\\n\\nTraining is complete!\")\n",
    "    \n",
    "    # Get the final accuracy scores by running the \"cost\" operation on the training and test data sets\n",
    "    final_training_cost = session.run(cost, feed_dict={X:X_scaled_training, Y:Y_scaled_training})\n",
    "    final_testing_cost = session.run(cost, feed_dict={X:X_scaled_testing, Y:Y_scaled_testing})\n",
    "    print (\"Final Training Cost:\",final_training_cost)\n",
    "    print (\"Final Testing Cost\", final_testing_cost)\n",
    "    \n",
    "    \n",
    "    # Now that the neural network is trained, let's use it to make predictions for our test data.\n",
    "    # Pass in the X testing data and run the \"prediciton\" operation\n",
    "    Y_predicted_scaled = session.run(prediction, feed_dict={X: X_scaled_testing})\n",
    "\n",
    "    # Unscale the data back to it's original units (dollars)\n",
    "    Y_predicted = Y_scaler.inverse_transform(Y_predicted_scaled)\n",
    "\n",
    "    real_earnings = test_data_df['total_earnings'].values[0]\n",
    "    predicted_earnings = Y_predicted[0][0]\n",
    "\n",
    "    print(\"The actual earnings of Game #1 were ${}\".format(real_earnings))\n",
    "    print(\"Our neural network predicted earnings of ${}\".format(predicted_earnings))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load data final.py\n",
    " - We are preloading all data into memory (RAM) as we have a smaller dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_training (1000, 9)\n",
      "Y_training (1000, 1)\n",
      "X_testing (400, 9)\n",
      "Y_testing (400, 1)\n",
      "Note: X[0] values were scaled by multiplying by 0.3333333333 and adding -0.6667\n",
      "Note: Y[0] values were scaled by multiplying by 0.0000036968 and adding -0.1159\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Load training data set from CSV file\n",
    "training_data_df = pd.read_csv('Exercise Files/03/sales_data_training.csv', dtype = float)\n",
    "\n",
    "# Pull out columns for X (data to train with) and Y (value to predict)\n",
    "X_training = training_data_df.drop('total_earnings', axis=1).values\n",
    "Y_training = training_data_df[['total_earnings']].values\n",
    "\n",
    "print('X_training', X_training.shape)\n",
    "print('Y_training', Y_training.shape)\n",
    "\n",
    "# Load testing data set from CSV file\n",
    "test_data_df =pd.read_csv('Exercise Files/03/sales_data_test.csv', dtype = float)\n",
    "\n",
    "# Pull out columns for X (data to train with) and Y (value to predict)\n",
    "X_testing = test_data_df.drop('total_earnings', axis=1).values\n",
    "Y_testing = test_data_df[['total_earnings']].values\n",
    "\n",
    "print('X_testing', X_testing.shape)\n",
    "print('Y_testing', Y_testing.shape)\n",
    "\n",
    "# All data needs to be scaled to a small range like 0 to 1 for the neural\n",
    "# network to work well. Create scalers for the inputs and outputs.\n",
    "X_scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "Y_scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "\n",
    "# Scale both the training inputs and outputs\n",
    "X_scaled_training = X_scaler.fit_transform(X_training)\n",
    "Y_scaled_training = Y_scaler.fit_transform(Y_training)\n",
    "\n",
    "# It's very important that the training and test data are scaled with the same scaler.\n",
    "# So, instead of fit_transform, we use transform to scale the test data using the 'min' & 'max' values found in training data\n",
    "X_scaled_testing = X_scaler.transform(X_testing)\n",
    "Y_scaled_testing = Y_scaler.transform(Y_testing)\n",
    "\n",
    "# We need to know the multiplication & addition factors to recover the original X from NN predictions later\n",
    "print(\"Note: X[0] values were scaled by multiplying by {:.10f} and adding {:.4f}\".format(X_scaler.scale_[0], X_scaler.min_[0]))\n",
    "print(\"Note: Y[0] values were scaled by multiplying by {:.10f} and adding {:.4f}\".format(Y_scaler.scale_[0], Y_scaler.min_[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **MinMaxScaler**\n",
    "<img src=\"Images/Min max scaler.png\" width=\"700\" height=\"400\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multiplication factor: 0.333333333333 Addition factor: -0.666666666667\n",
      "3.5 0.49996655 0.5\n",
      "X: 3.5\n",
      "X_max: 5.0\n",
      "X_min: 2.0\n",
      "max: 1\n",
      "min: 0\n",
      "X_std: 0.5\n",
      "X_scaled: 0.5\n"
     ]
    }
   ],
   "source": [
    "# Illustration on Min Max Scaler\n",
    "\n",
    "print('Multiplication factor:',X_scaler.scale_[0], 'Addition factor:',X_scaler.min_[0])\n",
    "print (X_training[0][0], X_training[0][0]*.3333333-.6667, X_scaled_training[0][0])\n",
    "\n",
    "# Detailed illustration\n",
    "X = X_training[0][0]\n",
    "X_max = X_training.max(axis=0)[0]\n",
    "X_min = X_training.min(axis=0)[0]\n",
    "feature_max = X_scaler.feature_range[1]\n",
    "feature_min = X_scaler.feature_range[0]\n",
    "\n",
    "print ('X:', X_training[0][0])\n",
    "print ('X_max:',X_training.max(axis=0)[0])\n",
    "print ('X_min:',X_training.min(axis=0)[0])\n",
    "print ('max:', X_scaler.feature_range[1])\n",
    "print ('min:', X_scaler.feature_range[0])\n",
    "\n",
    "X_std = (X - X_min)/(X_max-X_min)\n",
    "X_scaled = (X_std)*(feature_max-feature_min)+feature_min\n",
    "print ('X_std:', X_std)\n",
    "print ('X_scaled:', X_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## model final.py\n",
    "\n",
    "**Neural Network Design**\n",
    "<img src=\"Images/Neural Network Design 1.png\" width=\"600\" height=\"500\"> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_training (1000, 9)\n",
      "Y_training (1000, 1)\n",
      "X_testing (400, 9)\n",
      "Y_testing (400, 1)\n",
      "Note: X[0] values were scaled by multiplying by 0.3333333333 and adding -0.6667\n",
      "Note: Y[0] values were scaled by multiplying by 0.0000036968 and adding -0.1159\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Variable layer_1/weights1 does not exist, or was not created with tf.get_variable(). Did you mean to set reuse=tf.AUTO_REUSE in VarScope?",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-6fa7acb78dcf>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[1;31m# Layer 1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvariable_scope\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'layer_1'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreuse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 67\u001b[1;33m     \u001b[0mweights\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_variable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'weights1'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshape\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mnumber_of_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlayer_1_nodes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minitializer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mxavier_initializer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     68\u001b[0m     \u001b[0mbiases\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_variable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'biases1'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshape\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mlayer_1_nodes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minitializer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros_initializer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m     \u001b[0mlayer_1_output\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mweights\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mbiases\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\prasanth\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\u001b[0m in \u001b[0;36mget_variable\u001b[1;34m(name, shape, dtype, initializer, regularizer, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter, constraint)\u001b[0m\n\u001b[0;32m   1201\u001b[0m       \u001b[0mpartitioner\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpartitioner\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidate_shape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidate_shape\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1202\u001b[0m       \u001b[0muse_resource\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_resource\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcustom_getter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcustom_getter\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1203\u001b[1;33m       constraint=constraint)\n\u001b[0m\u001b[0;32m   1204\u001b[0m get_variable_or_local_docstring = (\n\u001b[0;32m   1205\u001b[0m     \"\"\"%s\n",
      "\u001b[1;32mc:\\users\\prasanth\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\u001b[0m in \u001b[0;36mget_variable\u001b[1;34m(self, var_store, name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter, constraint)\u001b[0m\n\u001b[0;32m   1090\u001b[0m           \u001b[0mpartitioner\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpartitioner\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidate_shape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidate_shape\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1091\u001b[0m           \u001b[0muse_resource\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_resource\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcustom_getter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcustom_getter\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1092\u001b[1;33m           constraint=constraint)\n\u001b[0m\u001b[0;32m   1093\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1094\u001b[0m   def _get_partitioned_variable(self,\n",
      "\u001b[1;32mc:\\users\\prasanth\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\u001b[0m in \u001b[0;36mget_variable\u001b[1;34m(self, name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter, constraint)\u001b[0m\n\u001b[0;32m    423\u001b[0m           \u001b[0mcaching_device\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcaching_device\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpartitioner\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpartitioner\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    424\u001b[0m           \u001b[0mvalidate_shape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidate_shape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0muse_resource\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_resource\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 425\u001b[1;33m           constraint=constraint)\n\u001b[0m\u001b[0;32m    426\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    427\u001b[0m   def _get_partitioned_variable(\n",
      "\u001b[1;32mc:\\users\\prasanth\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\u001b[0m in \u001b[0;36m_true_getter\u001b[1;34m(name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, constraint)\u001b[0m\n\u001b[0;32m    392\u001b[0m           \u001b[0mtrainable\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrainable\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcollections\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcollections\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    393\u001b[0m           \u001b[0mcaching_device\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcaching_device\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidate_shape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidate_shape\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 394\u001b[1;33m           use_resource=use_resource, constraint=constraint)\n\u001b[0m\u001b[0;32m    395\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    396\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mcustom_getter\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\prasanth\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py\u001b[0m in \u001b[0;36m_get_single_variable\u001b[1;34m(self, name, shape, dtype, initializer, regularizer, partition_info, reuse, trainable, collections, caching_device, validate_shape, use_resource, constraint)\u001b[0m\n\u001b[0;32m    758\u001b[0m       raise ValueError(\"Variable %s does not exist, or was not created with \"\n\u001b[0;32m    759\u001b[0m                        \u001b[1;34m\"tf.get_variable(). Did you mean to set \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 760\u001b[1;33m                        \"reuse=tf.AUTO_REUSE in VarScope?\" % name)\n\u001b[0m\u001b[0;32m    761\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mshape\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_fully_defined\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0minitializing_from_value\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    762\u001b[0m       raise ValueError(\"Shape of a new variable (%s) must be fully defined, \"\n",
      "\u001b[1;31mValueError\u001b[0m: Variable layer_1/weights1 does not exist, or was not created with tf.get_variable(). Did you mean to set reuse=tf.AUTO_REUSE in VarScope?"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Load training data set from CSV file\n",
    "training_data_df = pd.read_csv('Exercise Files/03/sales_data_training.csv', dtype = float)\n",
    "\n",
    "# Pull out columns for X (data to train with) and Y (value to predict)\n",
    "X_training = training_data_df.drop('total_earnings', axis=1).values\n",
    "Y_training = training_data_df[['total_earnings']].values\n",
    "\n",
    "print('X_training', X_training.shape)\n",
    "print('Y_training', Y_training.shape)\n",
    "\n",
    "# Load testing data set from CSV file\n",
    "test_data_df =pd.read_csv('Exercise Files/03/sales_data_test.csv', dtype = float)\n",
    "\n",
    "# Pull out columns for X (data to train with) and Y (value to predict)\n",
    "X_testing = test_data_df.drop('total_earnings', axis=1).values\n",
    "Y_testing = test_data_df[['total_earnings']].values\n",
    "\n",
    "print('X_testing', X_testing.shape)\n",
    "print('Y_testing', Y_testing.shape)\n",
    "\n",
    "# All data needs to be scaled to a small range like 0 to 1 for the neural\n",
    "# network to work well. Create scalers for the inputs and outputs.\n",
    "X_scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "Y_scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "\n",
    "# Scale both the training inputs and outputs\n",
    "X_scaled_training = X_scaler.fit_transform(X_training)\n",
    "Y_scaled_training = Y_scaler.fit_transform(Y_training)\n",
    "\n",
    "# It's very important that the training and test data are scaled with the same scaler.\n",
    "# So, instead of fit_transform, we use transform to scale the test data using the 'min' & 'max' values found in training data\n",
    "X_scaled_testing = X_scaler.transform(X_testing)\n",
    "Y_scaled_testing = Y_scaler.transform(Y_testing)\n",
    "\n",
    "# We need to know the multiplication & addition factors to recover the original X from NN predictions later\n",
    "print(\"Note: X[0] values were scaled by multiplying by {:.10f} and adding {:.4f}\".format(X_scaler.scale_[0], X_scaler.min_[0]))\n",
    "print(\"Note: Y[0] values were scaled by multiplying by {:.10f} and adding {:.4f}\".format(Y_scaler.scale_[0], Y_scaler.min_[0]))\n",
    "\n",
    "\n",
    "\n",
    "# Define model parameters\n",
    "learning_rate = 0.001\n",
    "training_epochs = 100\n",
    "display_step = 5\n",
    "\n",
    "# Define how many input and output nodes are in our neural network\n",
    "number_of_inputs = 9\n",
    "number_of_outputs = 1\n",
    "\n",
    "# Define how many neurons we want in each layer of our neural network\n",
    "layer_1_nodes = 50\n",
    "layer_2_nodes = 100\n",
    "layer_3_nodes = 50\n",
    "\n",
    "# Section One: Define the layers of the neural network itself\n",
    "\n",
    "# Input Layer\n",
    "with tf.variable_scope('input', reuse=True):\n",
    "    X = tf.placeholder(dtype=tf.float32, shape=(None,number_of_inputs))\n",
    "                                                # 'None' allows to accept batches of any size of inputs\n",
    "# Layer 1\n",
    "with tf.variable_scope('layer_1', reuse=True):\n",
    "    weights = tf.get_variable(name = 'weights1', shape = (number_of_inputs, layer_1_nodes), initializer=tf.contrib.layers.xavier_initializer())\n",
    "    biases = tf.get_variable(name = 'biases1', shape = (layer_1_nodes), initializer=tf.zeros_initializer())\n",
    "    layer_1_output = tf.nn.relu(tf.matmul(X,weights) + biases)\n",
    "\n",
    "# Layer 2\n",
    "with tf.variable_scope('layer_2', reuse=True):\n",
    "    weights = tf.get_variable(name = 'weights2', shape = (layer_1_nodes, layer_2_nodes), initializer=tf.contrib.layers.xavier_initializer())\n",
    "    biases = tf.get_variable(name = 'biases2', shape = (layer_2_nodes), initializer=tf.zeros_initializer())\n",
    "    layer_2_output = tf.nn.relu(tf.matmul(layer_1_output,weights) + biases)\n",
    "\n",
    "# Layer 3\n",
    "with tf.variable_scope('layer_3', reuse=True):\n",
    "    weights = tf.get_variable(name = 'weights3', shape = (layer_2_nodes, layer_3_nodes), initializer=tf.contrib.layers.xavier_initializer())\n",
    "    biases = tf.get_variable(name = 'biases3', shape = (layer_3_nodes), initializer=tf.zeros_initializer())\n",
    "    layer_3_output = tf.nn.relu(tf.matmul(layer_2_output,weights) + biases)\n",
    "\n",
    "# Output Layer\n",
    "with tf.variable_scope('output', reuse=True):\n",
    "    weights = tf.get_variable(name = 'weights4', shape = (layer_3_nodes, number_of_outputs), initializer=tf.contrib.layers.xavier_initializer())\n",
    "    biases = tf.get_variable(name = 'biases4', shape = (number_of_outputs), initializer=tf.zeros_initializer())\n",
    "    prediction = tf.nn.relu(tf.matmul(layer_3_output,weights) + biases)\n",
    "\n",
    "\n",
    "# Section Two: Define the cost function of the neural network that will measure prediction accuracy during training\n",
    "\n",
    "with tf.variable_scope('cost', reuse=True):\n",
    "    Y = tf.placeholder(dtype=tf.float32, shape=(None,number_of_outputs))\n",
    "    cost = tf.reduce_mean(tf.squared_difference(Y,prediction))\n",
    "\n",
    "\n",
    "# Section Three: Define the optimizer function that will be run to optimize the neural network\n",
    "\n",
    "with tf.variable_scope('train', reuse=True):\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate).minimize(cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## training loop final.py\n",
    "\n",
    "**<font color= blue>scope</font>**\n",
    "  - By putting nodes inside the scope, tf makes diagrams/ computational graph that are more easier to understand\n",
    "  - Everything within the same scope will be grouped together in the diagram\n",
    "  \n",
    "**<font color = blue>epoch</font>**\n",
    "  - *Another name for one full training pass over the training data*\n",
    "  - <font color = green>One epoch is one full run through the training data set</font>\n",
    "  - `training_epochs = 100`\n",
    "   - => We will do 100 iterations over the entire training dataset to train the Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_training (1000, 9)\n",
      "Y_training (1000, 1)\n",
      "X_testing (400, 9)\n",
      "Y_testing (400, 1)\n",
      "Note: X[0] values were scaled by multiplying by 0.3333333333 and adding -0.6667\n",
      "Note: Y[0] values were scaled by multiplying by 0.0000036968 and adding -0.1159\n",
      "Training pass: 0\n",
      "Training pass: 1\n",
      "Training pass: 2\n",
      "Training pass: 3\n",
      "Training pass: 4\n",
      "Training pass: 5\n",
      "Training pass: 6\n",
      "Training pass: 7\n",
      "Training pass: 8\n",
      "Training pass: 9\n",
      "Training pass: 10\n",
      "Training pass: 11\n",
      "Training pass: 12\n",
      "Training pass: 13\n",
      "Training pass: 14\n",
      "Training pass: 15\n",
      "Training pass: 16\n",
      "Training pass: 17\n",
      "Training pass: 18\n",
      "Training pass: 19\n",
      "Training pass: 20\n",
      "Training pass: 21\n",
      "Training pass: 22\n",
      "Training pass: 23\n",
      "Training pass: 24\n",
      "Training pass: 25\n",
      "Training pass: 26\n",
      "Training pass: 27\n",
      "Training pass: 28\n",
      "Training pass: 29\n",
      "Training pass: 30\n",
      "Training pass: 31\n",
      "Training pass: 32\n",
      "Training pass: 33\n",
      "Training pass: 34\n",
      "Training pass: 35\n",
      "Training pass: 36\n",
      "Training pass: 37\n",
      "Training pass: 38\n",
      "Training pass: 39\n",
      "Training pass: 40\n",
      "Training pass: 41\n",
      "Training pass: 42\n",
      "Training pass: 43\n",
      "Training pass: 44\n",
      "Training pass: 45\n",
      "Training pass: 46\n",
      "Training pass: 47\n",
      "Training pass: 48\n",
      "Training pass: 49\n",
      "Training pass: 50\n",
      "Training pass: 51\n",
      "Training pass: 52\n",
      "Training pass: 53\n",
      "Training pass: 54\n",
      "Training pass: 55\n",
      "Training pass: 56\n",
      "Training pass: 57\n",
      "Training pass: 58\n",
      "Training pass: 59\n",
      "Training pass: 60\n",
      "Training pass: 61\n",
      "Training pass: 62\n",
      "Training pass: 63\n",
      "Training pass: 64\n",
      "Training pass: 65\n",
      "Training pass: 66\n",
      "Training pass: 67\n",
      "Training pass: 68\n",
      "Training pass: 69\n",
      "Training pass: 70\n",
      "Training pass: 71\n",
      "Training pass: 72\n",
      "Training pass: 73\n",
      "Training pass: 74\n",
      "Training pass: 75\n",
      "Training pass: 76\n",
      "Training pass: 77\n",
      "Training pass: 78\n",
      "Training pass: 79\n",
      "Training pass: 80\n",
      "Training pass: 81\n",
      "Training pass: 82\n",
      "Training pass: 83\n",
      "Training pass: 84\n",
      "Training pass: 85\n",
      "Training pass: 86\n",
      "Training pass: 87\n",
      "Training pass: 88\n",
      "Training pass: 89\n",
      "Training pass: 90\n",
      "Training pass: 91\n",
      "Training pass: 92\n",
      "Training pass: 93\n",
      "Training pass: 94\n",
      "Training pass: 95\n",
      "Training pass: 96\n",
      "Training pass: 97\n",
      "Training pass: 98\n",
      "Training pass: 99\n",
      "Training is complete!\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Load training data set from CSV file\n",
    "training_data_df = pd.read_csv('Exercise Files/03/sales_data_training.csv', dtype = float)\n",
    "\n",
    "# Pull out columns for X (data to train with) and Y (value to predict)\n",
    "X_training = training_data_df.drop('total_earnings', axis=1).values\n",
    "Y_training = training_data_df[['total_earnings']].values\n",
    "\n",
    "print('X_training', X_training.shape)\n",
    "print('Y_training', Y_training.shape)\n",
    "\n",
    "# Load testing data set from CSV file\n",
    "test_data_df =pd.read_csv('Exercise Files/03/sales_data_test.csv', dtype = float)\n",
    "\n",
    "# Pull out columns for X (data to train with) and Y (value to predict)\n",
    "X_testing = test_data_df.drop('total_earnings', axis=1).values\n",
    "Y_testing = test_data_df[['total_earnings']].values\n",
    "\n",
    "print('X_testing', X_testing.shape)\n",
    "print('Y_testing', Y_testing.shape)\n",
    "\n",
    "# All data needs to be scaled to a small range like 0 to 1 for the neural\n",
    "# network to work well. Create scalers for the inputs and outputs.\n",
    "X_scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "Y_scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "\n",
    "# Scale both the training inputs and outputs\n",
    "X_scaled_training = X_scaler.fit_transform(X_training)\n",
    "Y_scaled_training = Y_scaler.fit_transform(Y_training)\n",
    "\n",
    "# It's very important that the training and test data are scaled with the same scaler.\n",
    "# So, instead of fit_transform, we use transform to scale the test data using the 'min' & 'max' values found in training data\n",
    "X_scaled_testing = X_scaler.transform(X_testing)\n",
    "Y_scaled_testing = Y_scaler.transform(Y_testing)\n",
    "\n",
    "# We need to know the multiplication & addition factors to recover the original X from NN predictions later\n",
    "print(\"Note: X[0] values were scaled by multiplying by {:.10f} and adding {:.4f}\".format(X_scaler.scale_[0], X_scaler.min_[0]))\n",
    "print(\"Note: Y[0] values were scaled by multiplying by {:.10f} and adding {:.4f}\".format(Y_scaler.scale_[0], Y_scaler.min_[0]))\n",
    "\n",
    "# Define model parameters\n",
    "learning_rate = 0.001\n",
    "training_epochs = 100\n",
    "display_step = 5\n",
    "\n",
    "# Define how many input and output nodes are in our neural network\n",
    "number_of_inputs = 9\n",
    "number_of_outputs = 1\n",
    "\n",
    "# Define how many neurons we want in each layer of our neural network\n",
    "layer_1_nodes = 50\n",
    "layer_2_nodes = 100\n",
    "layer_3_nodes = 50\n",
    "\n",
    "# Section One: Define the layers of the neural network itself\n",
    "\n",
    "# Input Layer\n",
    "with tf.variable_scope('input', reuse=True):\n",
    "    X = tf.placeholder(dtype=tf.float32, shape=(None,number_of_inputs))\n",
    "                                                # 'None' allows to accept batches of any size of inputs\n",
    "# Layer 1\n",
    "with tf.variable_scope('layer_1', reuse=True):\n",
    "    weights = tf.get_variable(name = 'weights1', shape = (number_of_inputs, layer_1_nodes), initializer=tf.contrib.layers.xavier_initializer())\n",
    "    biases = tf.get_variable(name = 'biases1', shape = (layer_1_nodes), initializer=tf.zeros_initializer())\n",
    "    layer_1_output = tf.nn.relu(tf.matmul(X,weights) + biases)\n",
    "\n",
    "# Layer 2\n",
    "with tf.variable_scope('layer_2', reuse=True):\n",
    "    weights = tf.get_variable(name = 'weights2', shape = (layer_1_nodes, layer_2_nodes), initializer=tf.contrib.layers.xavier_initializer())\n",
    "    biases = tf.get_variable(name = 'biases2', shape = (layer_2_nodes), initializer=tf.zeros_initializer())\n",
    "    layer_2_output = tf.nn.relu(tf.matmul(layer_1_output,weights) + biases)\n",
    "\n",
    "# Layer 3\n",
    "with tf.variable_scope('layer_3', reuse=True):\n",
    "    weights = tf.get_variable(name = 'weights3', shape = (layer_2_nodes, layer_3_nodes), initializer=tf.contrib.layers.xavier_initializer())\n",
    "    biases = tf.get_variable(name = 'biases3', shape = (layer_3_nodes), initializer=tf.zeros_initializer())\n",
    "    layer_3_output = tf.nn.relu(tf.matmul(layer_2_output,weights) + biases)\n",
    "\n",
    "# Output Layer\n",
    "with tf.variable_scope('output', reuse=True):\n",
    "    weights = tf.get_variable(name = 'weights4', shape = (layer_3_nodes, number_of_outputs), initializer=tf.contrib.layers.xavier_initializer())\n",
    "    biases = tf.get_variable(name = 'biases4', shape = (number_of_outputs), initializer=tf.zeros_initializer())\n",
    "    prediction = tf.nn.relu(tf.matmul(layer_3_output,weights) + biases)\n",
    "\n",
    "\n",
    "# Section Two: Define the cost function of the neural network that will measure prediction accuracy during training\n",
    "\n",
    "with tf.variable_scope('cost', reuse=True):\n",
    "    Y = tf.placeholder(dtype=tf.float32, shape=(None,number_of_outputs))\n",
    "    cost = tf.reduce_mean(tf.squared_difference(Y,prediction))\n",
    "\n",
    "\n",
    "# Section Three: Define the optimizer function that will be run to optimize the neural network\n",
    "\n",
    "with tf.variable_scope('train', reuse=True):\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate).minimize(cost)\n",
    "\n",
    "\n",
    "# Initialize a session so that we can run TensorFlow operations on the Graph\n",
    "with tf.Session() as session:\n",
    "\n",
    "    # Run the global variable initializer to initialize all variables and layers of the neural network\n",
    "    session.run(tf.global_variables_initializer())\n",
    "\n",
    "    # Run the optimizer over and over to train the network.\n",
    "    # One epoch is one full run through the training data set.\n",
    "    for epoch in range(training_epochs):\n",
    "        \n",
    "        # Feed in the training data and do one step of neural network training\n",
    "        session.run(optimizer, feed_dict={X:X_scaled_training, Y:Y_scaled_training})\n",
    "\n",
    "        # Print the current training status to the screen\n",
    "        print(\"Training pass: {}\".format(epoch))\n",
    "\n",
    "    # Training is now complete!\n",
    "    print(\"Training is complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a Model in TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## training final.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_training (1000, 9)\n",
      "Y_training (1000, 1)\n",
      "X_testing (400, 9)\n",
      "Y_testing (400, 1)\n",
      "Note: X[0] values were scaled by multiplying by 0.3333333333 and adding -0.6667\n",
      "Note: Y[0] values were scaled by multiplying by 0.0000036968 and adding -0.1159\n",
      "\n",
      "\n",
      "Training is started!\n",
      "0 0.0165624 0.0172908\n",
      "5 0.00655515 0.00726649\n",
      "10 0.00326075 0.00392983\n",
      "15 0.0018033 0.00218274\n",
      "20 0.00132281 0.00142179\n",
      "25 0.00088386 0.00093736\n",
      "30 0.000686801 0.000721912\n",
      "35 0.000543916 0.000587636\n",
      "40 0.000411467 0.000490671\n",
      "45 0.000314028 0.000388038\n",
      "50 0.000253952 0.000326844\n",
      "55 0.000211061 0.000273373\n",
      "60 0.000175134 0.000238316\n",
      "65 0.000147025 0.000221659\n",
      "70 0.000128935 0.000205763\n",
      "75 0.000113273 0.000187123\n",
      "80 0.000100511 0.000172974\n",
      "85 9.01683e-05 0.000161473\n",
      "90 8.14551e-05 0.000150954\n",
      "95 7.42242e-05 0.000142696\n",
      "\n",
      "\n",
      "Training is complete!\n",
      "Final Training Cost: 6.93672e-05\n",
      "Final Testing Cost 0.00013722\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Load training data set from CSV file\n",
    "training_data_df = pd.read_csv('Exercise Files/03/sales_data_training.csv', dtype = float)\n",
    "\n",
    "# Pull out columns for X (data to train with) and Y (value to predict)\n",
    "X_training = training_data_df.drop('total_earnings', axis=1).values\n",
    "Y_training = training_data_df[['total_earnings']].values\n",
    "\n",
    "print('X_training', X_training.shape)\n",
    "print('Y_training', Y_training.shape)\n",
    "\n",
    "# Load testing data set from CSV file\n",
    "test_data_df =pd.read_csv('Exercise Files/03/sales_data_test.csv', dtype = float)\n",
    "\n",
    "# Pull out columns for X (data to train with) and Y (value to predict)\n",
    "X_testing = test_data_df.drop('total_earnings', axis=1).values\n",
    "Y_testing = test_data_df[['total_earnings']].values\n",
    "\n",
    "print('X_testing', X_testing.shape)\n",
    "print('Y_testing', Y_testing.shape)\n",
    "\n",
    "# All data needs to be scaled to a small range like 0 to 1 for the neural\n",
    "# network to work well. Create scalers for the inputs and outputs.\n",
    "X_scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "Y_scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "\n",
    "# Scale both the training inputs and outputs\n",
    "X_scaled_training = X_scaler.fit_transform(X_training)\n",
    "Y_scaled_training = Y_scaler.fit_transform(Y_training)\n",
    "\n",
    "# It's very important that the training and test data are scaled with the same scaler.\n",
    "# So, instead of fit_transform, we use transform to scale the test data using the 'min' & 'max' values found in training data\n",
    "X_scaled_testing = X_scaler.transform(X_testing)\n",
    "Y_scaled_testing = Y_scaler.transform(Y_testing)\n",
    "\n",
    "# We need to know the multiplication & addition factors to recover the original X from NN predictions later\n",
    "print(\"Note: X[0] values were scaled by multiplying by {:.10f} and adding {:.4f}\".format(X_scaler.scale_[0], X_scaler.min_[0]))\n",
    "print(\"Note: Y[0] values were scaled by multiplying by {:.10f} and adding {:.4f}\".format(Y_scaler.scale_[0], Y_scaler.min_[0]))\n",
    "\n",
    "# Define model parameters\n",
    "learning_rate = 0.001\n",
    "training_epochs = 100\n",
    "display_step = 5\n",
    "\n",
    "# Define how many input and output nodes are in our neural network\n",
    "number_of_inputs = 9\n",
    "number_of_outputs = 1\n",
    "\n",
    "# Define how many neurons we want in each layer of our neural network\n",
    "layer_1_nodes = 50\n",
    "layer_2_nodes = 100\n",
    "layer_3_nodes = 50\n",
    "\n",
    "# Section One: Define the layers of the neural network itself\n",
    "\n",
    "# Input Layer\n",
    "with tf.variable_scope('input', reuse=True):\n",
    "    X = tf.placeholder(dtype=tf.float32, shape=(None,number_of_inputs))\n",
    "                                                # 'None' allows to accept batches of any size of inputs\n",
    "# Layer 1\n",
    "with tf.variable_scope('layer_1', reuse=True):\n",
    "    weights = tf.get_variable(name = 'weights1', shape = (number_of_inputs, layer_1_nodes), initializer=tf.contrib.layers.xavier_initializer())\n",
    "    biases = tf.get_variable(name = 'biases1', shape = (layer_1_nodes), initializer=tf.zeros_initializer())\n",
    "    layer_1_output = tf.nn.relu(tf.matmul(X,weights) + biases)\n",
    "\n",
    "# Layer 2\n",
    "with tf.variable_scope('layer_2', reuse=True):\n",
    "    weights = tf.get_variable(name = 'weights2', shape = (layer_1_nodes, layer_2_nodes), initializer=tf.contrib.layers.xavier_initializer())\n",
    "    biases = tf.get_variable(name = 'biases2', shape = (layer_2_nodes), initializer=tf.zeros_initializer())\n",
    "    layer_2_output = tf.nn.relu(tf.matmul(layer_1_output,weights) + biases)\n",
    "\n",
    "# Layer 3\n",
    "with tf.variable_scope('layer_3', reuse=True):\n",
    "    weights = tf.get_variable(name = 'weights3', shape = (layer_2_nodes, layer_3_nodes), initializer=tf.contrib.layers.xavier_initializer())\n",
    "    biases = tf.get_variable(name = 'biases3', shape = (layer_3_nodes), initializer=tf.zeros_initializer())\n",
    "    layer_3_output = tf.nn.relu(tf.matmul(layer_2_output,weights) + biases)\n",
    "\n",
    "# Output Layer\n",
    "with tf.variable_scope('output', reuse=True):\n",
    "    weights = tf.get_variable(name = 'weights4', shape = (layer_3_nodes, number_of_outputs), initializer=tf.contrib.layers.xavier_initializer())\n",
    "    biases = tf.get_variable(name = 'biases4', shape = (number_of_outputs), initializer=tf.zeros_initializer())\n",
    "    prediction = tf.nn.relu(tf.matmul(layer_3_output,weights) + biases)\n",
    "\n",
    "\n",
    "# Section Two: Define the cost function of the neural network that will measure prediction accuracy during training\n",
    "\n",
    "with tf.variable_scope('cost', reuse=True):\n",
    "    Y = tf.placeholder(dtype=tf.float32, shape=(None,number_of_outputs))\n",
    "    cost = tf.reduce_mean(tf.squared_difference(Y,prediction))\n",
    "\n",
    "\n",
    "# Section Three: Define the optimizer function that will be run to optimize the neural network\n",
    "\n",
    "with tf.variable_scope('train', reuse=True):\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate).minimize(cost)\n",
    "\n",
    "\n",
    "# Initialize a session so that we can run TensorFlow operations on the Graph\n",
    "with tf.Session() as session:\n",
    "\n",
    "    # Run the global variable initializer to initialize all variables and layers of the neural network\n",
    "    session.run(tf.global_variables_initializer())\n",
    "\n",
    "    # Run the optimizer over and over to train the network.\n",
    "    # One epoch is one full run through the training data set.\n",
    "    \n",
    "    print(\"\\n\\nTraining is started!\")\n",
    "    \n",
    "    for epoch in range(training_epochs):\n",
    "        \n",
    "        # Feed in the training data and do one step of neural network training\n",
    "        session.run(optimizer, feed_dict={X:X_scaled_training, Y:Y_scaled_training})\n",
    "\n",
    "        # Print the current training status to the screen\n",
    "        #print(\"Training pass: {}\".format(epoch))\n",
    "        \n",
    "        # Every 5 training steps, log our progress\n",
    "        if epoch%5 == 0:\n",
    "            training_cost = session.run(cost, feed_dict={X:X_scaled_training, Y:Y_scaled_training})\n",
    "            testing_cost = session.run(cost, feed_dict={X:X_scaled_testing, Y:Y_scaled_testing})\n",
    "            print (epoch, training_cost, testing_cost)\n",
    "            \n",
    "    # Training is now complete!\n",
    "    print(\"\\n\\nTraining is complete!\")\n",
    "    \n",
    "    final_training_cost = session.run(cost, feed_dict={X:X_scaled_training, Y:Y_scaled_training})\n",
    "    final_testing_cost = session.run(cost, feed_dict={X:X_scaled_testing, Y:Y_scaled_testing})\n",
    "    print (\"Final Training Cost:\",final_training_cost)\n",
    "    print (\"Final Testing Cost\", final_testing_cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## model logging final.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - To run **tensorboard**,\n",
    "   - In Terminal, \n",
    "       - For MaC, type `tensorboard --logdir=Exercise\\ Files/04/logs/` \n",
    "       - For Windows, type `tensorboard --logdir=\"Exercise Files\\04\\logs\"\\`\n",
    "   - In browser, \n",
    "       - For MAc, type http://0.0.0.0:6006/\n",
    "       - For Windows, type http://desktop-2v0tp7a:6006/\n",
    "   If the address doesn't work, look at the terminal window to find the corresponding address\n",
    "      <img src=\"Images/Tensorboard 1.png\" width=\"500\" height=\"250\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_training (1000, 9)\n",
      "Y_training (1000, 1)\n",
      "X_testing (400, 9)\n",
      "Y_testing (400, 1)\n",
      "Note: X[0] values were scaled by multiplying by 0.3333333333 and adding -0.6667\n",
      "Note: Y[0] values were scaled by multiplying by 0.0000036968 and adding -0.1159\n",
      "\n",
      "\n",
      "Training is started!\n",
      "Epoch: 0 - Training Cost: 0.06632847338914871,  Testing Cost: 0.07328135520219803\n",
      "Epoch: 5 - Training Cost: 0.03010611981153488,  Testing Cost: 0.030468640848994255\n",
      "Epoch: 10 - Training Cost: 0.015345592051744461,  Testing Cost: 0.01658996380865574\n",
      "Epoch: 15 - Training Cost: 0.013125012628734112,  Testing Cost: 0.01461787149310112\n",
      "Epoch: 20 - Training Cost: 0.00833351630717516,  Testing Cost: 0.009160040877759457\n",
      "Epoch: 25 - Training Cost: 0.007446942385286093,  Testing Cost: 0.008216545917093754\n",
      "Epoch: 30 - Training Cost: 0.00477180490270257,  Testing Cost: 0.005210546310991049\n",
      "Epoch: 35 - Training Cost: 0.00363171542994678,  Testing Cost: 0.004037527367472649\n",
      "Epoch: 40 - Training Cost: 0.002732048276811838,  Testing Cost: 0.003236934542655945\n",
      "Epoch: 45 - Training Cost: 0.0017697076546028256,  Testing Cost: 0.0020853986497968435\n",
      "Epoch: 50 - Training Cost: 0.001172793679870665,  Testing Cost: 0.0014871074818074703\n",
      "Epoch: 55 - Training Cost: 0.0008674244745634496,  Testing Cost: 0.001168449642136693\n",
      "Epoch: 60 - Training Cost: 0.000688431435264647,  Testing Cost: 0.0009693983010947704\n",
      "Epoch: 65 - Training Cost: 0.0005321426433511078,  Testing Cost: 0.0007612174958921969\n",
      "Epoch: 70 - Training Cost: 0.00042717886390164495,  Testing Cost: 0.000630841008387506\n",
      "Epoch: 75 - Training Cost: 0.00035535721690393984,  Testing Cost: 0.0005567634361796081\n",
      "Epoch: 80 - Training Cost: 0.0003016122500412166,  Testing Cost: 0.0004970204317942262\n",
      "Epoch: 85 - Training Cost: 0.00025817324058152735,  Testing Cost: 0.0004453136643860489\n",
      "Epoch: 90 - Training Cost: 0.00022104455274529755,  Testing Cost: 0.00041595459333620965\n",
      "Epoch: 95 - Training Cost: 0.0001920743816299364,  Testing Cost: 0.00038807137752883136\n",
      "\n",
      "\n",
      "Training is complete!\n",
      "Final Training Cost: 0.000173861\n",
      "Final Testing Cost 0.000377289\n",
      "The actual earnings of Game #1 were $247537.0\n",
      "Our neural network predicted earnings of $259961.5\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Load training data set from CSV file\n",
    "training_data_df = pd.read_csv('Exercise Files/03/sales_data_training.csv', dtype = float)\n",
    "\n",
    "# Pull out columns for X (data to train with) and Y (value to predict)\n",
    "X_training = training_data_df.drop('total_earnings', axis=1).values\n",
    "Y_training = training_data_df[['total_earnings']].values\n",
    "\n",
    "print('X_training', X_training.shape)\n",
    "print('Y_training', Y_training.shape)\n",
    "\n",
    "# Load testing data set from CSV file\n",
    "test_data_df =pd.read_csv('Exercise Files/03/sales_data_test.csv', dtype = float)\n",
    "\n",
    "# Pull out columns for X (data to train with) and Y (value to predict)\n",
    "X_testing = test_data_df.drop('total_earnings', axis=1).values\n",
    "Y_testing = test_data_df[['total_earnings']].values\n",
    "\n",
    "print('X_testing', X_testing.shape)\n",
    "print('Y_testing', Y_testing.shape)\n",
    "\n",
    "# All data needs to be scaled to a small range like 0 to 1 for the neural\n",
    "# network to work well. Create scalers for the inputs and outputs.\n",
    "X_scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "Y_scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "\n",
    "# Scale both the training inputs and outputs\n",
    "X_scaled_training = X_scaler.fit_transform(X_training)\n",
    "Y_scaled_training = Y_scaler.fit_transform(Y_training)\n",
    "\n",
    "# It's very important that the training and test data are scaled with the same scaler.\n",
    "# So, instead of fit_transform, we use transform to scale the test data using the 'min' & 'max' values found in training data\n",
    "X_scaled_testing = X_scaler.transform(X_testing)\n",
    "Y_scaled_testing = Y_scaler.transform(Y_testing)\n",
    "\n",
    "# We need to know the multiplication & addition factors to recover the original X from NN predictions later\n",
    "print(\"Note: X[0] values were scaled by multiplying by {:.10f} and adding {:.4f}\".format(X_scaler.scale_[0], X_scaler.min_[0]))\n",
    "print(\"Note: Y[0] values were scaled by multiplying by {:.10f} and adding {:.4f}\".format(Y_scaler.scale_[0], Y_scaler.min_[0]))\n",
    "\n",
    "# Define model parameters\n",
    "learning_rate = 0.001\n",
    "training_epochs = 100\n",
    "display_step = 5\n",
    "\n",
    "# Define how many input and output nodes are in our neural network\n",
    "number_of_inputs = 9\n",
    "number_of_outputs = 1\n",
    "\n",
    "# Define how many neurons we want in each layer of our neural network\n",
    "layer_1_nodes = 50\n",
    "layer_2_nodes = 100\n",
    "layer_3_nodes = 50\n",
    "\n",
    "# Section One: Define the layers of the neural network itself\n",
    "\n",
    "# Input Layer\n",
    "with tf.variable_scope('input'):\n",
    "    X = tf.placeholder(dtype=tf.float32, shape=(None,number_of_inputs))\n",
    "                                                # 'None' allows to accept batches of any size of inputs\n",
    "# Layer 1\n",
    "with tf.variable_scope('layer_1'):\n",
    "    weights = tf.get_variable(name = 'weights1', shape = (number_of_inputs, layer_1_nodes), initializer=tf.contrib.layers.xavier_initializer())\n",
    "    biases = tf.get_variable(name = 'biases1', shape = (layer_1_nodes), initializer=tf.zeros_initializer())\n",
    "    layer_1_output = tf.nn.relu(tf.matmul(X,weights) + biases)\n",
    "\n",
    "# Layer 2\n",
    "with tf.variable_scope('layer_2'):\n",
    "    weights = tf.get_variable(name = 'weights2', shape = (layer_1_nodes, layer_2_nodes), initializer=tf.contrib.layers.xavier_initializer())\n",
    "    biases = tf.get_variable(name = 'biases2', shape = (layer_2_nodes), initializer=tf.zeros_initializer())\n",
    "    layer_2_output = tf.nn.relu(tf.matmul(layer_1_output,weights) + biases)\n",
    "\n",
    "# Layer 3\n",
    "with tf.variable_scope('layer_3'):\n",
    "    weights = tf.get_variable(name = 'weights3', shape = (layer_2_nodes, layer_3_nodes), initializer=tf.contrib.layers.xavier_initializer())\n",
    "    biases = tf.get_variable(name = 'biases3', shape = (layer_3_nodes), initializer=tf.zeros_initializer())\n",
    "    layer_3_output = tf.nn.relu(tf.matmul(layer_2_output,weights) + biases)\n",
    "\n",
    "# Output Layer\n",
    "with tf.variable_scope('output'):\n",
    "    weights = tf.get_variable(name = 'weights4', shape = (layer_3_nodes, number_of_outputs), initializer=tf.contrib.layers.xavier_initializer())\n",
    "    biases = tf.get_variable(name = 'biases4', shape = (number_of_outputs), initializer=tf.zeros_initializer())\n",
    "    prediction = tf.nn.relu(tf.matmul(layer_3_output,weights) + biases)\n",
    "\n",
    "\n",
    "# Section Two: Define the cost function of the neural network that will measure prediction accuracy during training\n",
    "\n",
    "with tf.variable_scope('cost'):\n",
    "    Y = tf.placeholder(dtype=tf.float32, shape=(None,number_of_outputs))\n",
    "    cost = tf.reduce_mean(tf.squared_difference(Y,prediction))\n",
    "\n",
    "\n",
    "# Section Three: Define the optimizer function that will be run to optimize the neural network\n",
    "\n",
    "with tf.variable_scope('train'):\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate).minimize(cost)\n",
    "\n",
    "# Create a summary operation to log the progress of the network\n",
    "with tf.variable_scope('logging'):\n",
    "    tf.summary.scalar('current_cost', cost)\n",
    "    summary = tf.summary.merge_all()\n",
    "\n",
    "# Initialize a session so that we can run TensorFlow operations on the Graph\n",
    "with tf.Session() as session:\n",
    "\n",
    "    # Run the global variable initializer to initialize all variables and layers of the neural network\n",
    "    session.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Create log file writers to record training progress.\n",
    "    # We'll store training and testing log data separately.\n",
    "    training_writer = tf.summary.FileWriter(\"./Exercise Files/04/logs/training\", session.graph)\n",
    "    testing_writer = tf.summary.FileWriter(\"./Exercise Files/04/logs/testing\", session.graph)\n",
    "\n",
    "    # Run the optimizer over and over to train the network.\n",
    "    # One epoch is one full run through the training data set.\n",
    "    print(\"\\n\\nTraining is started!\")\n",
    "    \n",
    "    for epoch in range(training_epochs):\n",
    "        \n",
    "        # Feed in the training data and do one step of neural network training\n",
    "        session.run(optimizer, feed_dict={X:X_scaled_training, Y:Y_scaled_training})\n",
    "\n",
    "        # Print the current training status to the screen\n",
    "        #print(\"Training pass: {}\".format(epoch))\n",
    "        \n",
    "        # Every 5 training steps, log our progress\n",
    "        if epoch%5 == 0:\n",
    "            # Get the current accuracy scores by running the \"cost\" operation on the training and test data sets\n",
    "            training_cost, training_summary = session.run([cost, summary], feed_dict={X:X_scaled_training, Y:Y_scaled_training})\n",
    "            testing_cost, testing_summary = session.run([cost, summary], feed_dict={X:X_scaled_testing, Y:Y_scaled_testing})\n",
    "            \n",
    "            # Write the current training status to the log files (Which we can view with TensorBoard)\n",
    "            training_writer.add_summary(training_summary, epoch)   # Here, epoch is x axis & training_summary is y axis\n",
    "            testing_writer.add_summary(testing_summary, epoch)\n",
    "            \n",
    "            print(\"Epoch: {} - Training Cost: {},  Testing Cost: {}\".format(epoch, training_cost, testing_cost))\n",
    "\n",
    "            \n",
    "    # Training is now complete!\n",
    "    print(\"\\n\\nTraining is complete!\")\n",
    "    \n",
    "    # Get the final accuracy scores by running the \"cost\" operation on the training and test data sets\n",
    "    final_training_cost = session.run(cost, feed_dict={X:X_scaled_training, Y:Y_scaled_training})\n",
    "    final_testing_cost = session.run(cost, feed_dict={X:X_scaled_testing, Y:Y_scaled_testing})\n",
    "    print (\"Final Training Cost:\",final_training_cost)\n",
    "    print (\"Final Testing Cost\", final_testing_cost)\n",
    "    \n",
    "    \n",
    "    # Now that the neural network is trained, let's use it to make predictions for our test data.\n",
    "    # Pass in the X testing data and run the \"prediciton\" operation\n",
    "    Y_predicted_scaled = session.run(prediction, feed_dict={X: X_scaled_testing})\n",
    "\n",
    "    # Unscale the data back to it's original units (dollars)\n",
    "    Y_predicted = Y_scaler.inverse_transform(Y_predicted_scaled)\n",
    "\n",
    "    real_earnings = test_data_df['total_earnings'].values[0]\n",
    "    predicted_earnings = Y_predicted[0][0]\n",
    "\n",
    "    print(\"The actual earnings of Game #1 were ${}\".format(real_earnings))\n",
    "    print(\"Our neural network predicted earnings of ${}\".format(predicted_earnings))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving & Loading Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### model checkpoints final.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_training (1000, 9)\n",
      "Y_training (1000, 1)\n",
      "X_testing (400, 9)\n",
      "Y_testing (400, 1)\n",
      "Note: X[0] values were scaled by multiplying by 0.3333333333 and adding -0.6667\n",
      "Note: Y[0] values were scaled by multiplying by 0.0000036968 and adding -0.1159\n",
      "\n",
      "\n",
      "Training is started!\n",
      "Epoch: 0 - Training Cost: 0.040063560009002686,  Testing Cost: 0.04491918534040451\n",
      "Epoch: 5 - Training Cost: 0.025136245414614677,  Testing Cost: 0.02678118273615837\n",
      "Epoch: 10 - Training Cost: 0.011627444997429848,  Testing Cost: 0.013449461199343204\n",
      "Epoch: 15 - Training Cost: 0.01174045167863369,  Testing Cost: 0.013158433139324188\n",
      "Epoch: 20 - Training Cost: 0.007065911311656237,  Testing Cost: 0.007945246994495392\n",
      "Epoch: 25 - Training Cost: 0.005931700114160776,  Testing Cost: 0.006559228990226984\n",
      "Epoch: 30 - Training Cost: 0.004354249220341444,  Testing Cost: 0.005124305374920368\n",
      "Epoch: 35 - Training Cost: 0.002724509686231613,  Testing Cost: 0.003181713167577982\n",
      "Epoch: 40 - Training Cost: 0.0019441315671429038,  Testing Cost: 0.0021756647620350122\n",
      "Epoch: 45 - Training Cost: 0.001411033677868545,  Testing Cost: 0.0016511804424226284\n",
      "Epoch: 50 - Training Cost: 0.0010232332861050963,  Testing Cost: 0.0011210283264517784\n",
      "Epoch: 55 - Training Cost: 0.0008171084336936474,  Testing Cost: 0.0009818413527682424\n",
      "Epoch: 60 - Training Cost: 0.0006496776477433741,  Testing Cost: 0.0007478143088519573\n",
      "Epoch: 65 - Training Cost: 0.0005201898165978491,  Testing Cost: 0.0006487086648121476\n",
      "Epoch: 70 - Training Cost: 0.0004151163448113948,  Testing Cost: 0.0005379970534704626\n",
      "Epoch: 75 - Training Cost: 0.00034286751179024577,  Testing Cost: 0.00047758856089785695\n",
      "Epoch: 80 - Training Cost: 0.0002888631133828312,  Testing Cost: 0.0004086429544258863\n",
      "Epoch: 85 - Training Cost: 0.0002505724551156163,  Testing Cost: 0.0003665305266622454\n",
      "Epoch: 90 - Training Cost: 0.00022073631407693028,  Testing Cost: 0.00034212664468213916\n",
      "Epoch: 95 - Training Cost: 0.0001969125005416572,  Testing Cost: 0.00031664714333601296\n",
      "\n",
      "\n",
      "Training is complete!\n",
      "Final Training Cost: 0.000181578\n",
      "Final Testing Cost 0.000305282\n",
      "The actual earnings of Game #1 were $247537.0\n",
      "Our neural network predicted earnings of $244124.34375\n",
      "Model saved: ./Exercise Files/04/logs/trained_model.ckpt\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Load training data set from CSV file\n",
    "training_data_df = pd.read_csv('Exercise Files/03/sales_data_training.csv', dtype = float)\n",
    "\n",
    "# Pull out columns for X (data to train with) and Y (value to predict)\n",
    "X_training = training_data_df.drop('total_earnings', axis=1).values\n",
    "Y_training = training_data_df[['total_earnings']].values\n",
    "\n",
    "print('X_training', X_training.shape)\n",
    "print('Y_training', Y_training.shape)\n",
    "\n",
    "# Load testing data set from CSV file\n",
    "test_data_df =pd.read_csv('Exercise Files/03/sales_data_test.csv', dtype = float)\n",
    "\n",
    "# Pull out columns for X (data to train with) and Y (value to predict)\n",
    "X_testing = test_data_df.drop('total_earnings', axis=1).values\n",
    "Y_testing = test_data_df[['total_earnings']].values\n",
    "\n",
    "print('X_testing', X_testing.shape)\n",
    "print('Y_testing', Y_testing.shape)\n",
    "\n",
    "# All data needs to be scaled to a small range like 0 to 1 for the neural\n",
    "# network to work well. Create scalers for the inputs and outputs.\n",
    "X_scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "Y_scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "\n",
    "# Scale both the training inputs and outputs\n",
    "X_scaled_training = X_scaler.fit_transform(X_training)\n",
    "Y_scaled_training = Y_scaler.fit_transform(Y_training)\n",
    "\n",
    "# It's very important that the training and test data are scaled with the same scaler.\n",
    "# So, instead of fit_transform, we use transform to scale the test data using the 'min' & 'max' values found in training data\n",
    "X_scaled_testing = X_scaler.transform(X_testing)\n",
    "Y_scaled_testing = Y_scaler.transform(Y_testing)\n",
    "\n",
    "# We need to know the multiplication & addition factors to recover the original X from NN predictions later\n",
    "print(\"Note: X[0] values were scaled by multiplying by {:.10f} and adding {:.4f}\".format(X_scaler.scale_[0], X_scaler.min_[0]))\n",
    "print(\"Note: Y[0] values were scaled by multiplying by {:.10f} and adding {:.4f}\".format(Y_scaler.scale_[0], Y_scaler.min_[0]))\n",
    "\n",
    "# Define model parameters\n",
    "learning_rate = 0.001\n",
    "training_epochs = 100\n",
    "display_step = 5\n",
    "\n",
    "# Define how many input and output nodes are in our neural network\n",
    "number_of_inputs = 9\n",
    "number_of_outputs = 1\n",
    "\n",
    "# Define how many neurons we want in each layer of our neural network\n",
    "layer_1_nodes = 50\n",
    "layer_2_nodes = 100\n",
    "layer_3_nodes = 50\n",
    "\n",
    "# Section One: Define the layers of the neural network itself\n",
    "\n",
    "# Input Layer\n",
    "with tf.variable_scope('input'):\n",
    "    X = tf.placeholder(dtype=tf.float32, shape=(None,number_of_inputs))\n",
    "                                                # 'None' allows to accept batches of any size of inputs\n",
    "# Layer 1\n",
    "with tf.variable_scope('layer_1'):\n",
    "    weights = tf.get_variable(name = 'weights1', shape = (number_of_inputs, layer_1_nodes), initializer=tf.contrib.layers.xavier_initializer())\n",
    "    biases = tf.get_variable(name = 'biases1', shape = (layer_1_nodes), initializer=tf.zeros_initializer())\n",
    "    layer_1_output = tf.nn.relu(tf.matmul(X,weights) + biases)\n",
    "\n",
    "# Layer 2\n",
    "with tf.variable_scope('layer_2'):\n",
    "    weights = tf.get_variable(name = 'weights2', shape = (layer_1_nodes, layer_2_nodes), initializer=tf.contrib.layers.xavier_initializer())\n",
    "    biases = tf.get_variable(name = 'biases2', shape = (layer_2_nodes), initializer=tf.zeros_initializer())\n",
    "    layer_2_output = tf.nn.relu(tf.matmul(layer_1_output,weights) + biases)\n",
    "\n",
    "# Layer 3\n",
    "with tf.variable_scope('layer_3'):\n",
    "    weights = tf.get_variable(name = 'weights3', shape = (layer_2_nodes, layer_3_nodes), initializer=tf.contrib.layers.xavier_initializer())\n",
    "    biases = tf.get_variable(name = 'biases3', shape = (layer_3_nodes), initializer=tf.zeros_initializer())\n",
    "    layer_3_output = tf.nn.relu(tf.matmul(layer_2_output,weights) + biases)\n",
    "\n",
    "# Output Layer\n",
    "with tf.variable_scope('output'):\n",
    "    weights = tf.get_variable(name = 'weights4', shape = (layer_3_nodes, number_of_outputs), initializer=tf.contrib.layers.xavier_initializer())\n",
    "    biases = tf.get_variable(name = 'biases4', shape = (number_of_outputs), initializer=tf.zeros_initializer())\n",
    "    prediction = tf.nn.relu(tf.matmul(layer_3_output,weights) + biases)\n",
    "\n",
    "\n",
    "# Section Two: Define the cost function of the neural network that will measure prediction accuracy during training\n",
    "\n",
    "with tf.variable_scope('cost'):\n",
    "    Y = tf.placeholder(dtype=tf.float32, shape=(None,number_of_outputs))\n",
    "    cost = tf.reduce_mean(tf.squared_difference(Y,prediction))\n",
    "\n",
    "\n",
    "# Section Three: Define the optimizer function that will be run to optimize the neural network\n",
    "\n",
    "with tf.variable_scope('train'):\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate).minimize(cost)\n",
    "\n",
    "# Create a summary operation to log the progress of the network\n",
    "with tf.variable_scope('logging'):\n",
    "    tf.summary.scalar('current_cost', cost)\n",
    "    summary = tf.summary.merge_all()\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "# Initialize a session so that we can run TensorFlow operations on the Graph\n",
    "with tf.Session() as session:\n",
    "\n",
    "    # Run the global variable initializer to initialize all variables and layers of the neural network\n",
    "    session.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Create log file writers to record training progress.\n",
    "    # We'll store training and testing log data separately.\n",
    "    training_writer = tf.summary.FileWriter(\"./Exercise Files/04/logs/training\", session.graph)\n",
    "    testing_writer = tf.summary.FileWriter(\"./Exercise Files/04/logs/testing\", session.graph)\n",
    "\n",
    "    # Run the optimizer over and over to train the network.\n",
    "    # One epoch is one full run through the training data set.\n",
    "    print(\"\\n\\nTraining is started!\")\n",
    "    \n",
    "    for epoch in range(training_epochs):\n",
    "        \n",
    "        # Feed in the training data and do one step of neural network training\n",
    "        session.run(optimizer, feed_dict={X:X_scaled_training, Y:Y_scaled_training})\n",
    "\n",
    "        # Print the current training status to the screen\n",
    "        #print(\"Training pass: {}\".format(epoch))\n",
    "        \n",
    "        # Every 5 training steps, log our progress\n",
    "        if epoch%5 == 0:\n",
    "            # Get the current accuracy scores by running the \"cost\" operation on the training and test data sets\n",
    "            training_cost, training_summary = session.run([cost, summary], feed_dict={X:X_scaled_training, Y:Y_scaled_training})\n",
    "            testing_cost, testing_summary = session.run([cost, summary], feed_dict={X:X_scaled_testing, Y:Y_scaled_testing})\n",
    "            \n",
    "            # Write the current training status to the log files (Which we can view with TensorBoard)\n",
    "            training_writer.add_summary(training_summary, epoch)   # Here, epoch is x axis & training_summary is y axis\n",
    "            testing_writer.add_summary(testing_summary, epoch)\n",
    "            \n",
    "            print(\"Epoch: {} - Training Cost: {},  Testing Cost: {}\".format(epoch, training_cost, testing_cost))\n",
    "\n",
    "            \n",
    "    # Training is now complete!\n",
    "    print(\"\\n\\nTraining is complete!\")\n",
    "    \n",
    "    # Get the final accuracy scores by running the \"cost\" operation on the training and test data sets\n",
    "    final_training_cost = session.run(cost, feed_dict={X:X_scaled_training, Y:Y_scaled_training})\n",
    "    final_testing_cost = session.run(cost, feed_dict={X:X_scaled_testing, Y:Y_scaled_testing})\n",
    "    print (\"Final Training Cost:\",final_training_cost)\n",
    "    print (\"Final Testing Cost\", final_testing_cost)\n",
    "    \n",
    "    \n",
    "    # Now that the neural network is trained, let's use it to make predictions for our test data.\n",
    "    # Pass in the X testing data and run the \"prediciton\" operation\n",
    "    Y_predicted_scaled = session.run(prediction, feed_dict={X: X_scaled_testing})\n",
    "\n",
    "    # Unscale the data back to it's original units (dollars)\n",
    "    Y_predicted = Y_scaler.inverse_transform(Y_predicted_scaled)\n",
    "\n",
    "    real_earnings = test_data_df['total_earnings'].values[0]\n",
    "    predicted_earnings = Y_predicted[0][0]\n",
    "\n",
    "    print(\"The actual earnings of Game #1 were ${}\".format(real_earnings))\n",
    "    print(\"Our neural network predicted earnings of ${}\".format(predicted_earnings))\n",
    "    \n",
    "    save_path = saver.save(session, \"./Exercise Files/04/logs/trained_model.ckpt\")\n",
    "    print(\"Model saved: {}\".format(save_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load checkpoint.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_training (1000, 9)\n",
      "Y_training (1000, 1)\n",
      "X_testing (400, 9)\n",
      "Y_testing (400, 1)\n",
      "Note: X[0] values were scaled by multiplying by 0.3333333333 and adding -0.6667\n",
      "Note: Y[0] values were scaled by multiplying by 0.0000036968 and adding -0.1159\n",
      "INFO:tensorflow:Restoring parameters from ./Exercise Files/04/logs/trained_model.ckpt\n",
      "Final Training Cost: 0.000181578\n",
      "Final Testing Cost 0.000305282\n",
      "The actual earnings of Game #1 were $247537.0\n",
      "Our neural network predicted earnings of $244124.34375\n",
      "Model saved: ./Exercise Files/04/logs/trained_model.ckpt\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Load training data set from CSV file\n",
    "training_data_df = pd.read_csv('Exercise Files/04/sales_data_training.csv', dtype = float)\n",
    "\n",
    "# Pull out columns for X (data to train with) and Y (value to predict)\n",
    "X_training = training_data_df.drop('total_earnings', axis=1).values\n",
    "Y_training = training_data_df[['total_earnings']].values\n",
    "\n",
    "print('X_training', X_training.shape)\n",
    "print('Y_training', Y_training.shape)\n",
    "\n",
    "# Load testing data set from CSV file\n",
    "test_data_df =pd.read_csv('Exercise Files/04/sales_data_test.csv', dtype = float)\n",
    "\n",
    "# Pull out columns for X (data to train with) and Y (value to predict)\n",
    "X_testing = test_data_df.drop('total_earnings', axis=1).values\n",
    "Y_testing = test_data_df[['total_earnings']].values\n",
    "\n",
    "print('X_testing', X_testing.shape)\n",
    "print('Y_testing', Y_testing.shape)\n",
    "\n",
    "# All data needs to be scaled to a small range like 0 to 1 for the neural\n",
    "# network to work well. Create scalers for the inputs and outputs.\n",
    "X_scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "Y_scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "\n",
    "# Scale both the training inputs and outputs\n",
    "X_scaled_training = X_scaler.fit_transform(X_training)\n",
    "Y_scaled_training = Y_scaler.fit_transform(Y_training)\n",
    "\n",
    "# It's very important that the training and test data are scaled with the same scaler.\n",
    "# So, instead of fit_transform, we use transform to scale the test data using the 'min' & 'max' values found in training data\n",
    "X_scaled_testing = X_scaler.transform(X_testing)\n",
    "Y_scaled_testing = Y_scaler.transform(Y_testing)\n",
    "\n",
    "# We need to know the multiplication & addition factors to recover the original X from NN predictions later\n",
    "print(\"Note: X[0] values were scaled by multiplying by {:.10f} and adding {:.4f}\".format(X_scaler.scale_[0], X_scaler.min_[0]))\n",
    "print(\"Note: Y[0] values were scaled by multiplying by {:.10f} and adding {:.4f}\".format(Y_scaler.scale_[0], Y_scaler.min_[0]))\n",
    "\n",
    "# Define model parameters\n",
    "learning_rate = 0.001\n",
    "training_epochs = 100\n",
    "display_step = 5\n",
    "\n",
    "# Define how many input and output nodes are in our neural network\n",
    "number_of_inputs = 9\n",
    "number_of_outputs = 1\n",
    "\n",
    "# Define how many neurons we want in each layer of our neural network\n",
    "layer_1_nodes = 50\n",
    "layer_2_nodes = 100\n",
    "layer_3_nodes = 50\n",
    "\n",
    "# Section One: Define the layers of the neural network itself\n",
    "\n",
    "# Input Layer\n",
    "with tf.variable_scope('input'):\n",
    "    X = tf.placeholder(dtype=tf.float32, shape=(None,number_of_inputs))\n",
    "                                                # 'None' allows to accept batches of any size of inputs\n",
    "# Layer 1\n",
    "with tf.variable_scope('layer_1'):\n",
    "    weights = tf.get_variable(name = 'weights1', shape = (number_of_inputs, layer_1_nodes), initializer=tf.contrib.layers.xavier_initializer())\n",
    "    biases = tf.get_variable(name = 'biases1', shape = (layer_1_nodes), initializer=tf.zeros_initializer())\n",
    "    layer_1_output = tf.nn.relu(tf.matmul(X,weights) + biases)\n",
    "\n",
    "# Layer 2\n",
    "with tf.variable_scope('layer_2'):\n",
    "    weights = tf.get_variable(name = 'weights2', shape = (layer_1_nodes, layer_2_nodes), initializer=tf.contrib.layers.xavier_initializer())\n",
    "    biases = tf.get_variable(name = 'biases2', shape = (layer_2_nodes), initializer=tf.zeros_initializer())\n",
    "    layer_2_output = tf.nn.relu(tf.matmul(layer_1_output,weights) + biases)\n",
    "\n",
    "# Layer 3\n",
    "with tf.variable_scope('layer_3'):\n",
    "    weights = tf.get_variable(name = 'weights3', shape = (layer_2_nodes, layer_3_nodes), initializer=tf.contrib.layers.xavier_initializer())\n",
    "    biases = tf.get_variable(name = 'biases3', shape = (layer_3_nodes), initializer=tf.zeros_initializer())\n",
    "    layer_3_output = tf.nn.relu(tf.matmul(layer_2_output,weights) + biases)\n",
    "\n",
    "# Output Layer\n",
    "with tf.variable_scope('output'):\n",
    "    weights = tf.get_variable(name = 'weights4', shape = (layer_3_nodes, number_of_outputs), initializer=tf.contrib.layers.xavier_initializer())\n",
    "    biases = tf.get_variable(name = 'biases4', shape = (number_of_outputs), initializer=tf.zeros_initializer())\n",
    "    prediction = tf.nn.relu(tf.matmul(layer_3_output,weights) + biases)\n",
    "\n",
    "\n",
    "# Section Two: Define the cost function of the neural network that will measure prediction accuracy during training\n",
    "\n",
    "with tf.variable_scope('cost'):\n",
    "    Y = tf.placeholder(dtype=tf.float32, shape=(None,number_of_outputs))\n",
    "    cost = tf.reduce_mean(tf.squared_difference(Y,prediction))\n",
    "\n",
    "\n",
    "# Section Three: Define the optimizer function that will be run to optimize the neural network\n",
    "\n",
    "with tf.variable_scope('train'):\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate).minimize(cost)\n",
    "\n",
    "# Create a summary operation to log the progress of the network\n",
    "with tf.variable_scope('logging'):\n",
    "    tf.summary.scalar('current_cost', cost)\n",
    "    summary = tf.summary.merge_all()\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "# Initialize a session so that we can run TensorFlow operations on the Graph\n",
    "with tf.Session() as session:\n",
    "\n",
    "    # Instead, load the pretrained network from disk:\n",
    "    saver.restore(session, \"./Exercise Files/04/logs/trained_model.ckpt\")\n",
    "    \n",
    "    \n",
    "    ##### NO TRAINING LOOP ####\n",
    "    '''\n",
    "    # Run the global variable initializer to initialize all variables and layers of the neural network\n",
    "    # session.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Create log file writers to record training progress.\n",
    "    # We'll store training and testing log data separately.\n",
    "    training_writer = tf.summary.FileWriter(\"./Exercise Files/04/logs/training\", session.graph)\n",
    "    testing_writer = tf.summary.FileWriter(\"./Exercise Files/04/logs/testing\", session.graph)\n",
    "\n",
    "    # Run the optimizer over and over to train the network.\n",
    "    # One epoch is one full run through the training data set.\n",
    "    print(\"\\n\\nTraining is started!\")\n",
    "    \n",
    "    for epoch in range(training_epochs):\n",
    "        \n",
    "        # Feed in the training data and do one step of neural network training\n",
    "        session.run(optimizer, feed_dict={X:X_scaled_training, Y:Y_scaled_training})\n",
    "\n",
    "        # Print the current training status to the screen\n",
    "        #print(\"Training pass: {}\".format(epoch))\n",
    "        \n",
    "        # Every 5 training steps, log our progress\n",
    "        if epoch%5 == 0:\n",
    "            # Get the current accuracy scores by running the \"cost\" operation on the training and test data sets\n",
    "            training_cost, training_summary = session.run([cost, summary], feed_dict={X:X_scaled_training, Y:Y_scaled_training})\n",
    "            testing_cost, testing_summary = session.run([cost, summary], feed_dict={X:X_scaled_testing, Y:Y_scaled_testing})\n",
    "            \n",
    "            # Write the current training status to the log files (Which we can view with TensorBoard)\n",
    "            training_writer.add_summary(training_summary, epoch)   # Here, epoch is x axis & training_summary is y axis\n",
    "            testing_writer.add_summary(testing_summary, epoch)\n",
    "            \n",
    "            print(\"Epoch: {} - Training Cost: {},  Testing Cost: {}\".format(epoch, training_cost, testing_cost))\n",
    "\n",
    "            \n",
    "    # Training is now complete!\n",
    "    print(\"\\n\\nTraining is complete!\")\n",
    "    \n",
    "    '''\n",
    "    # Get the final accuracy scores by running the \"cost\" operation on the training and test data sets\n",
    "    final_training_cost = session.run(cost, feed_dict={X:X_scaled_training, Y:Y_scaled_training})\n",
    "    final_testing_cost = session.run(cost, feed_dict={X:X_scaled_testing, Y:Y_scaled_testing})\n",
    "    print (\"Final Training Cost:\",final_training_cost)\n",
    "    print (\"Final Testing Cost\", final_testing_cost)\n",
    "    \n",
    "    \n",
    "    # Now that the neural network is trained, let's use it to make predictions for our test data.\n",
    "    # Pass in the X testing data and run the \"prediciton\" operation\n",
    "    Y_predicted_scaled = session.run(prediction, feed_dict={X: X_scaled_testing})\n",
    "\n",
    "    # Unscale the data back to it's original units (dollars)\n",
    "    Y_predicted = Y_scaler.inverse_transform(Y_predicted_scaled)\n",
    "\n",
    "    real_earnings = test_data_df['total_earnings'].values[0]\n",
    "    predicted_earnings = Y_predicted[0][0]\n",
    "\n",
    "    print(\"The actual earnings of Game #1 were ${}\".format(real_earnings))\n",
    "    print(\"Our neural network predicted earnings of ${}\".format(predicted_earnings))\n",
    "    \n",
    "    save_path = saver.save(session, \"./Exercise Files/04/logs/trained_model.ckpt\")\n",
    "    print(\"Model saved: {}\".format(save_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorBoard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train_model.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **TensorBoard** \n",
    "    - <font color = green> Creates a graphical/visual representation of what we are doing in TensorFlow \n",
    "    - Great tool for exploring the structure of the model & understanding how data flows </font>\n",
    "    <br><br>\n",
    "- To run **tensorboard**,\n",
    "   - In Terminal, type `tensorboard --logdir=Exercise\\ Files/05/logs/` \n",
    "   - In browser, type http://0.0.0.0:6006/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_training (1000, 9)\n",
      "Y_training (1000, 1)\n",
      "X_testing (400, 9)\n",
      "Y_testing (400, 1)\n",
      "Note: X[0] values were scaled by multiplying by 0.3333333333 and adding -0.6667\n",
      "Note: Y[0] values were scaled by multiplying by 0.0000036968 and adding -0.1159\n",
      "\n",
      "\n",
      "Training is started!\n",
      "Epoch: 0 - Training Cost: 0.09586505591869354,  Testing Cost: 0.10329286754131317\n",
      "Epoch: 5 - Training Cost: 0.022488124668598175,  Testing Cost: 0.024101004004478455\n",
      "Epoch: 10 - Training Cost: 0.022535158321261406,  Testing Cost: 0.023592716082930565\n",
      "Epoch: 15 - Training Cost: 0.011481529101729393,  Testing Cost: 0.01324673555791378\n",
      "Epoch: 20 - Training Cost: 0.011240127496421337,  Testing Cost: 0.012610015459358692\n",
      "Epoch: 25 - Training Cost: 0.0065397159196436405,  Testing Cost: 0.006929340772330761\n",
      "Epoch: 30 - Training Cost: 0.005028821527957916,  Testing Cost: 0.00515239080414176\n",
      "Epoch: 35 - Training Cost: 0.00331862922757864,  Testing Cost: 0.0036836822982877493\n",
      "Epoch: 40 - Training Cost: 0.0019391147652640939,  Testing Cost: 0.0020848496351391077\n",
      "Epoch: 45 - Training Cost: 0.0014877858338877559,  Testing Cost: 0.0015255424659699202\n",
      "Epoch: 50 - Training Cost: 0.0010535325855016708,  Testing Cost: 0.0010938291670754552\n",
      "Epoch: 55 - Training Cost: 0.0008015755447559059,  Testing Cost: 0.0008295915904454887\n",
      "Epoch: 60 - Training Cost: 0.0006335933576337993,  Testing Cost: 0.0006673049065284431\n",
      "Epoch: 65 - Training Cost: 0.0005239878664724529,  Testing Cost: 0.0005793267046101391\n",
      "Epoch: 70 - Training Cost: 0.0004281372530385852,  Testing Cost: 0.0004927241243422031\n",
      "Epoch: 75 - Training Cost: 0.00034834479447454214,  Testing Cost: 0.0004047387919854373\n",
      "Epoch: 80 - Training Cost: 0.00028881008620373905,  Testing Cost: 0.0003488234360702336\n",
      "Epoch: 85 - Training Cost: 0.0002453971828799695,  Testing Cost: 0.00031065408256836236\n",
      "Epoch: 90 - Training Cost: 0.00021066601038910449,  Testing Cost: 0.0002814150939229876\n",
      "Epoch: 95 - Training Cost: 0.00018280636868439615,  Testing Cost: 0.0002557618136052042\n",
      "\n",
      "\n",
      "Training is complete!\n",
      "Final Training Cost: 0.000165268\n",
      "Final Testing Cost 0.000239265\n",
      "The actual earnings of Game #1 were $247537.0\n",
      "Our neural network predicted earnings of $247558.5\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Load training data set from CSV file\n",
    "training_data_df = pd.read_csv('Exercise Files/05/sales_data_training.csv', dtype = float)\n",
    "\n",
    "# Pull out columns for X (data to train with) and Y (value to predict)\n",
    "X_training = training_data_df.drop('total_earnings', axis=1).values\n",
    "Y_training = training_data_df[['total_earnings']].values\n",
    "\n",
    "print('X_training', X_training.shape)\n",
    "print('Y_training', Y_training.shape)\n",
    "\n",
    "# Load testing data set from CSV file\n",
    "test_data_df =pd.read_csv('Exercise Files/05/sales_data_test.csv', dtype = float)\n",
    "\n",
    "# Pull out columns for X (data to train with) and Y (value to predict)\n",
    "X_testing = test_data_df.drop('total_earnings', axis=1).values\n",
    "Y_testing = test_data_df[['total_earnings']].values\n",
    "\n",
    "print('X_testing', X_testing.shape)\n",
    "print('Y_testing', Y_testing.shape)\n",
    "\n",
    "# All data needs to be scaled to a small range like 0 to 1 for the neural\n",
    "# network to work well. Create scalers for the inputs and outputs.\n",
    "X_scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "Y_scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "\n",
    "# Scale both the training inputs and outputs\n",
    "X_scaled_training = X_scaler.fit_transform(X_training)\n",
    "Y_scaled_training = Y_scaler.fit_transform(Y_training)\n",
    "\n",
    "# It's very important that the training and test data are scaled with the same scaler.\n",
    "# So, instead of fit_transform, we use transform to scale the test data using the 'min' & 'max' values found in training data\n",
    "X_scaled_testing = X_scaler.transform(X_testing)\n",
    "Y_scaled_testing = Y_scaler.transform(Y_testing)\n",
    "\n",
    "# We need to know the multiplication & addition factors to recover the original X from NN predictions later\n",
    "print(\"Note: X[0] values were scaled by multiplying by {:.10f} and adding {:.4f}\".format(X_scaler.scale_[0], X_scaler.min_[0]))\n",
    "print(\"Note: Y[0] values were scaled by multiplying by {:.10f} and adding {:.4f}\".format(Y_scaler.scale_[0], Y_scaler.min_[0]))\n",
    "\n",
    "# Define model parameters\n",
    "learning_rate = 0.001\n",
    "training_epochs = 100\n",
    "display_step = 5\n",
    "\n",
    "# Define how many input and output nodes are in our neural network\n",
    "number_of_inputs = 9\n",
    "number_of_outputs = 1\n",
    "\n",
    "# Define how many neurons we want in each layer of our neural network\n",
    "layer_1_nodes = 50\n",
    "layer_2_nodes = 100\n",
    "layer_3_nodes = 50\n",
    "\n",
    "# Section One: Define the layers of the neural network itself\n",
    "\n",
    "# Input Layer\n",
    "with tf.variable_scope('input'):\n",
    "    X = tf.placeholder(dtype=tf.float32, shape=(None,number_of_inputs))\n",
    "                                                # 'None' allows to accept batches of any size of inputs\n",
    "# Layer 1\n",
    "with tf.variable_scope('layer_1'):\n",
    "    weights = tf.get_variable(name = 'weights1', shape = (number_of_inputs, layer_1_nodes), initializer=tf.contrib.layers.xavier_initializer())\n",
    "    biases = tf.get_variable(name = 'biases1', shape = (layer_1_nodes), initializer=tf.zeros_initializer())\n",
    "    layer_1_output = tf.nn.relu(tf.matmul(X,weights) + biases)\n",
    "\n",
    "# Layer 2\n",
    "with tf.variable_scope('layer_2'):\n",
    "    weights = tf.get_variable(name = 'weights2', shape = (layer_1_nodes, layer_2_nodes), initializer=tf.contrib.layers.xavier_initializer())\n",
    "    biases = tf.get_variable(name = 'biases2', shape = (layer_2_nodes), initializer=tf.zeros_initializer())\n",
    "    layer_2_output = tf.nn.relu(tf.matmul(layer_1_output,weights) + biases)\n",
    "\n",
    "# Layer 3\n",
    "with tf.variable_scope('layer_3'):\n",
    "    weights = tf.get_variable(name = 'weights3', shape = (layer_2_nodes, layer_3_nodes), initializer=tf.contrib.layers.xavier_initializer())\n",
    "    biases = tf.get_variable(name = 'biases3', shape = (layer_3_nodes), initializer=tf.zeros_initializer())\n",
    "    layer_3_output = tf.nn.relu(tf.matmul(layer_2_output,weights) + biases)\n",
    "\n",
    "# Output Layer\n",
    "with tf.variable_scope('output'):\n",
    "    weights = tf.get_variable(name = 'weights4', shape = (layer_3_nodes, number_of_outputs), initializer=tf.contrib.layers.xavier_initializer())\n",
    "    biases = tf.get_variable(name = 'biases4', shape = (number_of_outputs), initializer=tf.zeros_initializer())\n",
    "    prediction = tf.nn.relu(tf.matmul(layer_3_output,weights) + biases)\n",
    "\n",
    "\n",
    "# Section Two: Define the cost function of the neural network that will measure prediction accuracy during training\n",
    "\n",
    "with tf.variable_scope('cost'):\n",
    "    Y = tf.placeholder(dtype=tf.float32, shape=(None,number_of_outputs))\n",
    "    cost = tf.reduce_mean(tf.squared_difference(Y,prediction))\n",
    "\n",
    "\n",
    "# Section Three: Define the optimizer function that will be run to optimize the neural network\n",
    "\n",
    "with tf.variable_scope('train'):\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate).minimize(cost)\n",
    "\n",
    "# Create a summary operation to log the progress of the network\n",
    "with tf.variable_scope('logging'):\n",
    "    tf.summary.scalar('current_cost', cost)\n",
    "    summary = tf.summary.merge_all()\n",
    "\n",
    "# Initialize a session so that we can run TensorFlow operations on the Graph\n",
    "with tf.Session() as session:\n",
    "\n",
    "    # Run the global variable initializer to initialize all variables and layers of the neural network\n",
    "    session.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Create log file writers to record training progress.\n",
    "    # We'll store training and testing log data separately.\n",
    "    training_writer = tf.summary.FileWriter(\"./Exercise Files/05/logs/training\", session.graph)\n",
    "    testing_writer = tf.summary.FileWriter(\"./Exercise Files/05/logs/testing\", session.graph)\n",
    "\n",
    "    # Run the optimizer over and over to train the network.\n",
    "    # One epoch is one full run through the training data set.\n",
    "    print(\"\\n\\nTraining is started!\")\n",
    "    \n",
    "    for epoch in range(training_epochs):\n",
    "        \n",
    "        # Feed in the training data and do one step of neural network training\n",
    "        session.run(optimizer, feed_dict={X:X_scaled_training, Y:Y_scaled_training})\n",
    "\n",
    "        # Print the current training status to the screen\n",
    "        #print(\"Training pass: {}\".format(epoch))\n",
    "        \n",
    "        # Every 5 training steps, log our progress\n",
    "        if epoch%5 == 0:\n",
    "            # Get the current accuracy scores by running the \"cost\" operation on the training and test data sets\n",
    "            training_cost, training_summary = session.run([cost, summary], feed_dict={X:X_scaled_training, Y:Y_scaled_training})\n",
    "            testing_cost, testing_summary = session.run([cost, summary], feed_dict={X:X_scaled_testing, Y:Y_scaled_testing})\n",
    "            \n",
    "            # Write the current training status to the log files (Which we can view with TensorBoard)\n",
    "            training_writer.add_summary(training_summary, epoch)   # Here, epoch is x axis & training_summary is y axis\n",
    "            testing_writer.add_summary(testing_summary, epoch)\n",
    "            \n",
    "            print(\"Epoch: {} - Training Cost: {},  Testing Cost: {}\".format(epoch, training_cost, testing_cost))\n",
    "\n",
    "            \n",
    "    # Training is now complete!\n",
    "    print(\"\\n\\nTraining is complete!\")\n",
    "    \n",
    "    # Get the final accuracy scores by running the \"cost\" operation on the training and test data sets\n",
    "    final_training_cost = session.run(cost, feed_dict={X:X_scaled_training, Y:Y_scaled_training})\n",
    "    final_testing_cost = session.run(cost, feed_dict={X:X_scaled_testing, Y:Y_scaled_testing})\n",
    "    print (\"Final Training Cost:\",final_training_cost)\n",
    "    print (\"Final Testing Cost\", final_testing_cost)\n",
    "    \n",
    "    \n",
    "    # Now that the neural network is trained, let's use it to make predictions for our test data.\n",
    "    # Pass in the X testing data and run the \"prediciton\" operation\n",
    "    Y_predicted_scaled = session.run(prediction, feed_dict={X: X_scaled_testing})\n",
    "\n",
    "    # Unscale the data back to it's original units (dollars)\n",
    "    Y_predicted = Y_scaler.inverse_transform(Y_predicted_scaled)\n",
    "\n",
    "    real_earnings = test_data_df['total_earnings'].values[0]\n",
    "    predicted_earnings = Y_predicted[0][0]\n",
    "\n",
    "    print(\"The actual earnings of Game #1 were ${}\".format(real_earnings))\n",
    "    print(\"Our neural network predicted earnings of ${}\".format(predicted_earnings))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualise the Computational Graph\n",
    "- Each variable scope/ operation in NN is represented by a box in the graph\n",
    "- Boxes in same blue color => Same internal structure\n",
    "    - Layer 1, Layer 2, Layer 3\n",
    "- Each line between node represents 'Tensors' or 'Array of data' passed between nodes\n",
    "    - Input layer passes 9 values to layer 1 as input has 9 features\n",
    "    - The '?' represents the batch size\n",
    "        -  It varies each time\n",
    "    - Output layer passes only 1 value which is the predicted 'Total earnings'\n",
    "    <img src=\"Images/Computational Graph 3.png\" width=\"700\" height=\"950\">\n",
    "    \n",
    "    \n",
    " - **Deeper look into nodes**\n",
    "    - `Input` Node \n",
    "        - Has 1 placeholder for input, X\n",
    "    - `Layer_1` Node\n",
    "        - Has 2 variable nodes `weights_1` & `biases_1`\n",
    "        - Has `MatMul` performing matrix multiplication of (`X, weights_1`)\n",
    "        - Has `Add` performing addition of `biases_1` with `MatMul` results\n",
    "        - Has `relu` performing sigmoid operation on the result of `Add`\n",
    "    <img src=\"Images/Tensorboard graph zoom in 1.png\" width=\"700\" height=\"950\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## visulize_training.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- <font color = red> Till now, when we write the current training status  (i.e. cost of training & testing data) to the log files Which we can view with TensorBoard, additional log fils will be created with the same name as the old log files. The new log file mix with old log file creating overlap in tensorboards which makes it difficult to tell which models output is which </font>\n",
    "- To fix this, we can put the each log files into a new folder\n",
    "    - Delete any existing log files\n",
    "    - Add each log file into a new different folder\n",
    "    \n",
    " <img src=\"Images/Tensorboard 2.png\" width=\"700\" height=\"950\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_training (1000, 9)\n",
      "Y_training (1000, 1)\n",
      "X_testing (400, 9)\n",
      "Y_testing (400, 1)\n",
      "Note: X[0] values were scaled by multiplying by 0.3333333333 and adding -0.6667\n",
      "Note: Y[0] values were scaled by multiplying by 0.0000036968 and adding -0.1159\n",
      "\n",
      "\n",
      "Training is started!\n",
      "Epoch: 0 - Training Cost: 0.11315354704856873,  Testing Cost: 0.12232767790555954\n",
      "Epoch: 5 - Training Cost: 0.050370994955301285,  Testing Cost: 0.054224468767642975\n",
      "Epoch: 10 - Training Cost: 0.035928864032030106,  Testing Cost: 0.03752647712826729\n",
      "Epoch: 15 - Training Cost: 0.012860646471381187,  Testing Cost: 0.014562326483428478\n",
      "Epoch: 20 - Training Cost: 0.011996336281299591,  Testing Cost: 0.013960335403680801\n",
      "Epoch: 25 - Training Cost: 0.005443112459033728,  Testing Cost: 0.00600275257602334\n",
      "Epoch: 30 - Training Cost: 0.004086550325155258,  Testing Cost: 0.004309411160647869\n",
      "Epoch: 35 - Training Cost: 0.0035237858537584543,  Testing Cost: 0.0042927260510623455\n",
      "Epoch: 40 - Training Cost: 0.0020217758137732744,  Testing Cost: 0.0022465798538178205\n",
      "Epoch: 45 - Training Cost: 0.0013712623622268438,  Testing Cost: 0.0016045323573052883\n",
      "Epoch: 50 - Training Cost: 0.001169289811514318,  Testing Cost: 0.0014884743141010404\n",
      "Epoch: 55 - Training Cost: 0.0009409399935975671,  Testing Cost: 0.0011115059023723006\n",
      "Epoch: 60 - Training Cost: 0.0007513926830142736,  Testing Cost: 0.0010242502903565764\n",
      "Epoch: 65 - Training Cost: 0.0006205712561495602,  Testing Cost: 0.0008198767900466919\n",
      "Epoch: 70 - Training Cost: 0.0005392374587245286,  Testing Cost: 0.000753117143176496\n",
      "Epoch: 75 - Training Cost: 0.0004833128477912396,  Testing Cost: 0.0006758302333764732\n",
      "Epoch: 80 - Training Cost: 0.0004381163162179291,  Testing Cost: 0.0006132191047072411\n",
      "Epoch: 85 - Training Cost: 0.0003984105715062469,  Testing Cost: 0.0005721505731344223\n",
      "Epoch: 90 - Training Cost: 0.000363280123565346,  Testing Cost: 0.0005204341723583639\n",
      "Epoch: 95 - Training Cost: 0.0003318315139040351,  Testing Cost: 0.000486519536934793\n",
      "\n",
      "\n",
      "Training is complete!\n",
      "Final Training Cost: 0.000309297\n",
      "Final Testing Cost 0.000448335\n",
      "The actual earnings of Game #1 were $247537.0\n",
      "Our neural network predicted earnings of $247034.828125\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Load training data set from CSV file\n",
    "training_data_df = pd.read_csv('Exercise Files/05/sales_data_training.csv', dtype = float)\n",
    "\n",
    "# Pull out columns for X (data to train with) and Y (value to predict)\n",
    "X_training = training_data_df.drop('total_earnings', axis=1).values\n",
    "Y_training = training_data_df[['total_earnings']].values\n",
    "\n",
    "print('X_training', X_training.shape)\n",
    "print('Y_training', Y_training.shape)\n",
    "\n",
    "# Load testing data set from CSV file\n",
    "test_data_df =pd.read_csv('Exercise Files/05/sales_data_test.csv', dtype = float)\n",
    "\n",
    "# Pull out columns for X (data to train with) and Y (value to predict)\n",
    "X_testing = test_data_df.drop('total_earnings', axis=1).values\n",
    "Y_testing = test_data_df[['total_earnings']].values\n",
    "\n",
    "print('X_testing', X_testing.shape)\n",
    "print('Y_testing', Y_testing.shape)\n",
    "\n",
    "# All data needs to be scaled to a small range like 0 to 1 for the neural\n",
    "# network to work well. Create scalers for the inputs and outputs.\n",
    "X_scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "Y_scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "\n",
    "# Scale both the training inputs and outputs\n",
    "X_scaled_training = X_scaler.fit_transform(X_training)\n",
    "Y_scaled_training = Y_scaler.fit_transform(Y_training)\n",
    "\n",
    "# It's very important that the training and test data are scaled with the same scaler.\n",
    "# So, instead of fit_transform, we use transform to scale the test data using the 'min' & 'max' values found in training data\n",
    "X_scaled_testing = X_scaler.transform(X_testing)\n",
    "Y_scaled_testing = Y_scaler.transform(Y_testing)\n",
    "\n",
    "# We need to know the multiplication & addition factors to recover the original X from NN predictions later\n",
    "print(\"Note: X[0] values were scaled by multiplying by {:.10f} and adding {:.4f}\".format(X_scaler.scale_[0], X_scaler.min_[0]))\n",
    "print(\"Note: Y[0] values were scaled by multiplying by {:.10f} and adding {:.4f}\".format(Y_scaler.scale_[0], Y_scaler.min_[0]))\n",
    "\n",
    "# Define model parameters\n",
    "RUN_NAME = \"run 2 layer 1 20 nodes\"\n",
    "learning_rate = 0.001\n",
    "training_epochs = 100\n",
    "display_step = 5\n",
    "\n",
    "# Define how many input and output nodes are in our neural network\n",
    "number_of_inputs = 9\n",
    "number_of_outputs = 1\n",
    "\n",
    "# Define how many neurons we want in each layer of our neural network\n",
    "layer_1_nodes = 50\n",
    "layer_2_nodes = 100\n",
    "layer_3_nodes = 50\n",
    "\n",
    "# Section One: Define the layers of the neural network itself\n",
    "\n",
    "# Input Layer\n",
    "with tf.variable_scope('input'):\n",
    "    X = tf.placeholder(dtype=tf.float32, shape=(None,number_of_inputs))\n",
    "                                                # 'None' allows to accept batches of any size of inputs\n",
    "# Layer 1\n",
    "with tf.variable_scope('layer_1'):\n",
    "    weights = tf.get_variable(name = 'weights1', shape = (number_of_inputs, layer_1_nodes), initializer=tf.contrib.layers.xavier_initializer())\n",
    "    biases = tf.get_variable(name = 'biases1', shape = (layer_1_nodes), initializer=tf.zeros_initializer())\n",
    "    layer_1_output = tf.nn.relu(tf.matmul(X,weights) + biases)\n",
    "\n",
    "# Layer 2\n",
    "with tf.variable_scope('layer_2'):\n",
    "    weights = tf.get_variable(name = 'weights2', shape = (layer_1_nodes, layer_2_nodes), initializer=tf.contrib.layers.xavier_initializer())\n",
    "    biases = tf.get_variable(name = 'biases2', shape = (layer_2_nodes), initializer=tf.zeros_initializer())\n",
    "    layer_2_output = tf.nn.relu(tf.matmul(layer_1_output,weights) + biases)\n",
    "\n",
    "# Layer 3\n",
    "with tf.variable_scope('layer_3'):\n",
    "    weights = tf.get_variable(name = 'weights3', shape = (layer_2_nodes, layer_3_nodes), initializer=tf.contrib.layers.xavier_initializer())\n",
    "    biases = tf.get_variable(name = 'biases3', shape = (layer_3_nodes), initializer=tf.zeros_initializer())\n",
    "    layer_3_output = tf.nn.relu(tf.matmul(layer_2_output,weights) + biases)\n",
    "\n",
    "# Output Layer\n",
    "with tf.variable_scope('output'):\n",
    "    weights = tf.get_variable(name = 'weights4', shape = (layer_3_nodes, number_of_outputs), initializer=tf.contrib.layers.xavier_initializer())\n",
    "    biases = tf.get_variable(name = 'biases4', shape = (number_of_outputs), initializer=tf.zeros_initializer())\n",
    "    prediction = tf.nn.relu(tf.matmul(layer_3_output,weights) + biases)\n",
    "\n",
    "\n",
    "# Section Two: Define the cost function of the neural network that will measure prediction accuracy during training\n",
    "\n",
    "with tf.variable_scope('cost'):\n",
    "    Y = tf.placeholder(dtype=tf.float32, shape=(None,number_of_outputs))\n",
    "    cost = tf.reduce_mean(tf.squared_difference(Y,prediction))\n",
    "\n",
    "\n",
    "# Section Three: Define the optimizer function that will be run to optimize the neural network\n",
    "\n",
    "with tf.variable_scope('train'):\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate).minimize(cost)\n",
    "\n",
    "# Create a summary operation to log the progress of the network\n",
    "with tf.variable_scope('logging'):\n",
    "    tf.summary.scalar('current_cost', cost)\n",
    "    summary = tf.summary.merge_all()\n",
    "\n",
    "# Initialize a session so that we can run TensorFlow operations on the Graph\n",
    "with tf.Session() as session:\n",
    "\n",
    "    # Run the global variable initializer to initialize all variables and layers of the neural network\n",
    "    session.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Create log file writers to record training progress.\n",
    "    # We'll store training and testing log data separately.\n",
    "    training_writer = tf.summary.FileWriter(\"./Exercise Files/05/logs/{}/training\".format(RUN_NAME), session.graph)\n",
    "    testing_writer = tf.summary.FileWriter(\"./Exercise Files/05/logs/{}/testing\".format(RUN_NAME), session.graph)\n",
    "\n",
    "    # Run the optimizer over and over to train the network.\n",
    "    # One epoch is one full run through the training data set.\n",
    "    print(\"\\n\\nTraining is started!\")\n",
    "    \n",
    "    for epoch in range(training_epochs):\n",
    "        \n",
    "        # Feed in the training data and do one step of neural network training\n",
    "        session.run(optimizer, feed_dict={X:X_scaled_training, Y:Y_scaled_training})\n",
    "\n",
    "        # Print the current training status to the screen\n",
    "        #print(\"Training pass: {}\".format(epoch))\n",
    "        \n",
    "        # Every 5 training steps, log our progress\n",
    "        if epoch%display_step == 0:\n",
    "            # Get the current accuracy scores by running the \"cost\" operation on the training and test data sets\n",
    "            training_cost, training_summary = session.run([cost, summary], feed_dict={X:X_scaled_training, Y:Y_scaled_training})\n",
    "            testing_cost, testing_summary = session.run([cost, summary], feed_dict={X:X_scaled_testing, Y:Y_scaled_testing})\n",
    "            \n",
    "            # Write the current training status to the log files (Which we can view with TensorBoard)\n",
    "            training_writer.add_summary(training_summary, epoch)   # Here, epoch is x axis & training_summary is y axis\n",
    "            testing_writer.add_summary(testing_summary, epoch)\n",
    "            \n",
    "            print(\"Epoch: {} - Training Cost: {},  Testing Cost: {}\".format(epoch, training_cost, testing_cost))\n",
    "\n",
    "            \n",
    "    # Training is now complete!\n",
    "    print(\"\\n\\nTraining is complete!\")\n",
    "    \n",
    "    # Get the final accuracy scores by running the \"cost\" operation on the training and test data sets\n",
    "    final_training_cost = session.run(cost, feed_dict={X:X_scaled_training, Y:Y_scaled_training})\n",
    "    final_testing_cost = session.run(cost, feed_dict={X:X_scaled_testing, Y:Y_scaled_testing})\n",
    "    print (\"Final Training Cost:\",final_training_cost)\n",
    "    print (\"Final Testing Cost\", final_testing_cost)\n",
    "    \n",
    "    \n",
    "    # Now that the neural network is trained, let's use it to make predictions for our test data.\n",
    "    # Pass in the X testing data and run the \"prediciton\" operation\n",
    "    Y_predicted_scaled = session.run(prediction, feed_dict={X: X_scaled_testing})\n",
    "\n",
    "    # Unscale the data back to it's original units (dollars)\n",
    "    Y_predicted = Y_scaler.inverse_transform(Y_predicted_scaled)\n",
    "\n",
    "    real_earnings = test_data_df['total_earnings'].values[0]\n",
    "    predicted_earnings = Y_predicted[0][0]\n",
    "\n",
    "    print(\"The actual earnings of Game #1 were ${}\".format(real_earnings))\n",
    "    print(\"Our neural network predicted earnings of ${}\".format(predicted_earnings))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## custom_viz.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<font color = blue> Histograms & Distribution Charts </font>**\n",
    " - To see the histogram of the predictions made on training & test data *over the epochs/ iterations*, add this code into the  `tf.summary.histogram('predicted_value', prediction)` into the `tf.variable_scope('logging')`\n",
    " \n",
    "  <img src=\"Images/Tensorboard Histogram 1.png\" width=\"700\" height=\"500\">\n",
    "    <img src=\"Images/Tensorboard Distribution 1.png\" width=\"700\" height=\"500\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_training (1000, 9)\n",
      "Y_training (1000, 1)\n",
      "X_testing (400, 9)\n",
      "Y_testing (400, 1)\n",
      "Note: X[0] values were scaled by multiplying by 0.3333333333 and adding -0.6667\n",
      "Note: Y[0] values were scaled by multiplying by 0.0000036968 and adding -0.1159\n",
      "\n",
      "\n",
      "Training is started!\n",
      "Epoch: 0 - Training Cost: 0.10583586990833282,  Testing Cost: 0.11570969223976135\n",
      "Epoch: 5 - Training Cost: 0.018660148605704308,  Testing Cost: 0.02059137262403965\n",
      "Epoch: 10 - Training Cost: 0.0223349928855896,  Testing Cost: 0.022560857236385345\n",
      "Epoch: 15 - Training Cost: 0.009685941971838474,  Testing Cost: 0.010708744637668133\n",
      "Epoch: 20 - Training Cost: 0.010876830667257309,  Testing Cost: 0.012119731865823269\n",
      "Epoch: 25 - Training Cost: 0.006357858888804913,  Testing Cost: 0.007040110882371664\n",
      "Epoch: 30 - Training Cost: 0.005656322930008173,  Testing Cost: 0.006023681256920099\n",
      "Epoch: 35 - Training Cost: 0.0036118116695433855,  Testing Cost: 0.00401306850835681\n",
      "Epoch: 40 - Training Cost: 0.0033197100274264812,  Testing Cost: 0.0037179281935095787\n",
      "Epoch: 45 - Training Cost: 0.002351721515879035,  Testing Cost: 0.0026212341617792845\n",
      "Epoch: 50 - Training Cost: 0.0018017098773270845,  Testing Cost: 0.002031131414696574\n",
      "Epoch: 55 - Training Cost: 0.001484381384216249,  Testing Cost: 0.0017203869065269828\n",
      "Epoch: 60 - Training Cost: 0.0011495831422507763,  Testing Cost: 0.0013328850036486983\n",
      "Epoch: 65 - Training Cost: 0.0008917718078009784,  Testing Cost: 0.0010844566859304905\n",
      "Epoch: 70 - Training Cost: 0.0007117597269825637,  Testing Cost: 0.0008443599217571318\n",
      "Epoch: 75 - Training Cost: 0.0005857138894498348,  Testing Cost: 0.0007127398857846856\n",
      "Epoch: 80 - Training Cost: 0.0004841804038733244,  Testing Cost: 0.0006193545996211469\n",
      "Epoch: 85 - Training Cost: 0.0004099257639609277,  Testing Cost: 0.0005523943691514432\n",
      "Epoch: 90 - Training Cost: 0.000345640757586807,  Testing Cost: 0.0004864853108301759\n",
      "Epoch: 95 - Training Cost: 0.00029370200354605913,  Testing Cost: 0.0004332230309955776\n",
      "\n",
      "\n",
      "Training is complete!\n",
      "Final Training Cost: 0.000260135\n",
      "Final Testing Cost 0.000400182\n",
      "The actual earnings of Game #1 were $247537.0\n",
      "Our neural network predicted earnings of $242556.4375\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Load training data set from CSV file\n",
    "training_data_df = pd.read_csv('Exercise Files/05/sales_data_training.csv', dtype = float)\n",
    "\n",
    "# Pull out columns for X (data to train with) and Y (value to predict)\n",
    "X_training = training_data_df.drop('total_earnings', axis=1).values\n",
    "Y_training = training_data_df[['total_earnings']].values\n",
    "\n",
    "print('X_training', X_training.shape)\n",
    "print('Y_training', Y_training.shape)\n",
    "\n",
    "# Load testing data set from CSV file\n",
    "test_data_df =pd.read_csv('Exercise Files/05/sales_data_test.csv', dtype = float)\n",
    "\n",
    "# Pull out columns for X (data to train with) and Y (value to predict)\n",
    "X_testing = test_data_df.drop('total_earnings', axis=1).values\n",
    "Y_testing = test_data_df[['total_earnings']].values\n",
    "\n",
    "print('X_testing', X_testing.shape)\n",
    "print('Y_testing', Y_testing.shape)\n",
    "\n",
    "# All data needs to be scaled to a small range like 0 to 1 for the neural\n",
    "# network to work well. Create scalers for the inputs and outputs.\n",
    "X_scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "Y_scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "\n",
    "# Scale both the training inputs and outputs\n",
    "X_scaled_training = X_scaler.fit_transform(X_training)\n",
    "Y_scaled_training = Y_scaler.fit_transform(Y_training)\n",
    "\n",
    "# It's very important that the training and test data are scaled with the same scaler.\n",
    "# So, instead of fit_transform, we use transform to scale the test data using the 'min' & 'max' values found in training data\n",
    "X_scaled_testing = X_scaler.transform(X_testing)\n",
    "Y_scaled_testing = Y_scaler.transform(Y_testing)\n",
    "\n",
    "# We need to know the multiplication & addition factors to recover the original X from NN predictions later\n",
    "print(\"Note: X[0] values were scaled by multiplying by {:.10f} and adding {:.4f}\".format(X_scaler.scale_[0], X_scaler.min_[0]))\n",
    "print(\"Note: Y[0] values were scaled by multiplying by {:.10f} and adding {:.4f}\".format(Y_scaler.scale_[0], Y_scaler.min_[0]))\n",
    "\n",
    "# Define model parameters\n",
    "RUN_NAME = '' #\"run 2 layer 1 20 nodes\"\n",
    "learning_rate = 0.001\n",
    "training_epochs = 100\n",
    "display_step = 5\n",
    "\n",
    "# Define how many input and output nodes are in our neural network\n",
    "number_of_inputs = 9\n",
    "number_of_outputs = 1\n",
    "\n",
    "# Define how many neurons we want in each layer of our neural network\n",
    "layer_1_nodes = 50\n",
    "layer_2_nodes = 100\n",
    "layer_3_nodes = 50\n",
    "\n",
    "# Section One: Define the layers of the neural network itself\n",
    "\n",
    "# Input Layer\n",
    "with tf.variable_scope('input'):\n",
    "    X = tf.placeholder(dtype=tf.float32, shape=(None,number_of_inputs))\n",
    "                                                # 'None' allows to accept batches of any size of inputs\n",
    "# Layer 1\n",
    "with tf.variable_scope('layer_1'):\n",
    "    weights = tf.get_variable(name = 'weights1', shape = (number_of_inputs, layer_1_nodes), initializer=tf.contrib.layers.xavier_initializer())\n",
    "    biases = tf.get_variable(name = 'biases1', shape = (layer_1_nodes), initializer=tf.zeros_initializer())\n",
    "    layer_1_output = tf.nn.relu(tf.matmul(X,weights) + biases)\n",
    "\n",
    "# Layer 2\n",
    "with tf.variable_scope('layer_2'):\n",
    "    weights = tf.get_variable(name = 'weights2', shape = (layer_1_nodes, layer_2_nodes), initializer=tf.contrib.layers.xavier_initializer())\n",
    "    biases = tf.get_variable(name = 'biases2', shape = (layer_2_nodes), initializer=tf.zeros_initializer())\n",
    "    layer_2_output = tf.nn.relu(tf.matmul(layer_1_output,weights) + biases)\n",
    "\n",
    "# Layer 3\n",
    "with tf.variable_scope('layer_3'):\n",
    "    weights = tf.get_variable(name = 'weights3', shape = (layer_2_nodes, layer_3_nodes), initializer=tf.contrib.layers.xavier_initializer())\n",
    "    biases = tf.get_variable(name = 'biases3', shape = (layer_3_nodes), initializer=tf.zeros_initializer())\n",
    "    layer_3_output = tf.nn.relu(tf.matmul(layer_2_output,weights) + biases)\n",
    "\n",
    "# Output Layer\n",
    "with tf.variable_scope('output'):\n",
    "    weights = tf.get_variable(name = 'weights4', shape = (layer_3_nodes, number_of_outputs), initializer=tf.contrib.layers.xavier_initializer())\n",
    "    biases = tf.get_variable(name = 'biases4', shape = (number_of_outputs), initializer=tf.zeros_initializer())\n",
    "    prediction = tf.nn.relu(tf.matmul(layer_3_output,weights) + biases)\n",
    "\n",
    "\n",
    "# Section Two: Define the cost function of the neural network that will measure prediction accuracy during training\n",
    "\n",
    "with tf.variable_scope('cost'):\n",
    "    Y = tf.placeholder(dtype=tf.float32, shape=(None,number_of_outputs))\n",
    "    cost = tf.reduce_mean(tf.squared_difference(Y,prediction))\n",
    "\n",
    "\n",
    "# Section Three: Define the optimizer function that will be run to optimize the neural network\n",
    "\n",
    "with tf.variable_scope('train'):\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate).minimize(cost)\n",
    "\n",
    "# Create a summary operation to log the progress of the network\n",
    "with tf.variable_scope('logging'):\n",
    "    tf.summary.scalar('current_cost', cost)\n",
    "    tf.summary.histogram('predicted_value', prediction)\n",
    "    summary = tf.summary.merge_all()\n",
    "\n",
    "# Initialize a session so that we can run TensorFlow operations on the Graph\n",
    "with tf.Session() as session:\n",
    "\n",
    "    # Run the global variable initializer to initialize all variables and layers of the neural network\n",
    "    session.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Create log file writers to record training progress.\n",
    "    # We'll store training and testing log data separately.\n",
    "    training_writer = tf.summary.FileWriter(\"./Exercise Files/05/logs/{}/training\".format(RUN_NAME), session.graph)\n",
    "    testing_writer = tf.summary.FileWriter(\"./Exercise Files/05/logs/{}/testing\".format(RUN_NAME), session.graph)\n",
    "\n",
    "    # Run the optimizer over and over to train the network.\n",
    "    # One epoch is one full run through the training data set.\n",
    "    print(\"\\n\\nTraining is started!\")\n",
    "    \n",
    "    for epoch in range(training_epochs):\n",
    "        \n",
    "        # Feed in the training data and do one step of neural network training\n",
    "        session.run(optimizer, feed_dict={X:X_scaled_training, Y:Y_scaled_training})\n",
    "\n",
    "        # Print the current training status to the screen\n",
    "        #print(\"Training pass: {}\".format(epoch))\n",
    "        \n",
    "        # Every 5 training steps, log our progress\n",
    "        if epoch%display_step == 0:\n",
    "            # Get the current accuracy scores by running the \"cost\" operation on the training and test data sets\n",
    "            training_cost, training_summary = session.run([cost, summary], feed_dict={X:X_scaled_training, Y:Y_scaled_training})\n",
    "            testing_cost, testing_summary = session.run([cost, summary], feed_dict={X:X_scaled_testing, Y:Y_scaled_testing})\n",
    "            \n",
    "            # Write the current training status to the log files (Which we can view with TensorBoard)\n",
    "            training_writer.add_summary(training_summary, epoch)   # Here, epoch is x axis & training_summary is y axis\n",
    "            testing_writer.add_summary(testing_summary, epoch)\n",
    "            \n",
    "            print(\"Epoch: {} - Training Cost: {},  Testing Cost: {}\".format(epoch, training_cost, testing_cost))\n",
    "\n",
    "            \n",
    "    # Training is now complete!\n",
    "    print(\"\\n\\nTraining is complete!\")\n",
    "    \n",
    "    # Get the final accuracy scores by running the \"cost\" operation on the training and test data sets\n",
    "    final_training_cost = session.run(cost, feed_dict={X:X_scaled_training, Y:Y_scaled_training})\n",
    "    final_testing_cost = session.run(cost, feed_dict={X:X_scaled_testing, Y:Y_scaled_testing})\n",
    "    print (\"Final Training Cost:\",final_training_cost)\n",
    "    print (\"Final Testing Cost\", final_testing_cost)\n",
    "    \n",
    "    \n",
    "    # Now that the neural network is trained, let's use it to make predictions for our test data.\n",
    "    # Pass in the X testing data and run the \"prediciton\" operation\n",
    "    Y_predicted_scaled = session.run(prediction, feed_dict={X: X_scaled_testing})\n",
    "\n",
    "    # Unscale the data back to it's original units (dollars)\n",
    "    Y_predicted = Y_scaler.inverse_transform(Y_predicted_scaled)\n",
    "\n",
    "    real_earnings = test_data_df['total_earnings'].values[0]\n",
    "    predicted_earnings = Y_predicted[0][0]\n",
    "\n",
    "    print(\"The actual earnings of Game #1 were ${}\".format(real_earnings))\n",
    "    print(\"Our neural network predicted earnings of ${}\".format(predicted_earnings))\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": true,
   "toc_position": {
    "height": "884px",
    "left": "0px",
    "right": "1111px",
    "top": "107px",
    "width": "318px"
   },
   "toc_section_display": "block",
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
